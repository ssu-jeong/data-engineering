## 03. 매물별사용자조회수실습-실버데이터생성

---

지난 시간에 aws그루에 데이터카탈로그 서비스를 활용해서 hive테이블을 하나 생성하고 거기에 우리가 로우데이터(브론즈데이터)를 정제해 실버데이터로 만들었다.

이번 시간에는 그 아파트 단지내에 판매할 수 있는 매물단위(단지보다 디테일한 단위)를 조회한 사용자 수에 대해 실버데이터를 만들어 보자.

우리가 이제 어떤 비지니스 영역을 하든 보통 사용자, 아이템, 사용자와 아이템에 관련된 여러 데이터를 생성해 놓는게 보통 일반적인 데이터 분석에 있어서 기본적인 내용이다.


### AWS Glue를 활용한 Hive table 생성

---

![5-3-1](https://user-images.githubusercontent.com/86764734/157051413-3cfbf16b-e731-4f2b-8cf3-ee4a65cf956a.png)


1. 제플린으로 이동
    
    - 매물별 사용자조회수에 대해 실습 진행

    - 실습할 소스코드실행

![5-3-2](https://user-images.githubusercontent.com/86764734/157051681-7a5b7a6a-ec79-417a-b292-f0d7c478cfc4.png)


```
%sql

show tables
```
- 여기에 보면 tablename: logs라고 되어 있고 isTemporay는 true라고 되어 있을 것.

- 우리가 다른 노트북에서 생성해놓은 temporay 테이블은 이 세션이 끝나기 전까지는 살아 있다. 그래서 다른 노트북, zeppelin의 세션이 살아 있는 한은 조회해서 바로 분석할 수 있는 환경으로 사용해도 된다.

- 만약 없을 경우에는 첫번째 단지별 사용자 조회수에 가서 첫번째 logs까지 만드는 셀을 실행하고 두번째 실습코드를 진행하면 된다. 꼭 테이블을 확인하고 진행하자!

![5-3-3](https://user-images.githubusercontent.com/86764734/157052217-96aaa9f8-9c4a-4029-8bd4-73cd0b462375.png)

여기 까지 소스코드 실행해서 두번째 노트북으로 들어오란 소리!

2. 첫번째로 아파트의 매물에 해당되는 조건을 찾을 것!
    
    먼저 아이템 카탈로그가 ‘아파트' 이면서 스크린이름이 ‘아파트시세정보상세', ’아파트매물상세'의 두가지 경우 
    
    그리고 거기에 따른 매일(base_date), 어떤 사용자가(adid), 언제(dase_dt→시분초까지있는), 매물아이다(item_id), 단지에 대한 아이디(danji_id)

![5-3-4](https://user-images.githubusercontent.com/86764734/157052681-b16c9a2e-0016-4766-a7af-eed4bfdb3d27.png)

해당 소스코드 실행하면  

📍 단지 아이디가 0인 것 → 의미가 무엇일까? 이런 것들은 분석 담당자들과 커뮤니케이션을 통해 해당 의미를 파악해 나가야 한다.

![5-3-5](https://user-images.githubusercontent.com/86764734/157052807-365eb76c-4461-4ac1-ad91-aec53d3e90fc.png)

우선은 쿼리를 아파트 매물별 사용자가 조회한 결과로 생각할 것.
이것을 저장하기 위해 glue 데이터카탈로그에 테이블을 생성하자!

4. Glue에 테이블 정의
    
    - 클래스라는 데이터베이스에 단지별 유저 아이템뷰 테이블(danji_user_item_view_sliver)을 만들어 준다.
    
    - 항상 뒤에 있는 s3에 저장하는 폴더명도 이 테이블 명하고 똑같이 가져가는게 나중에 s3에서 이 데이터를 찾을 때 좀 더 쉽게 찾을 수 있다. 
    
    - 스토어 유형은 parquet 유형으로 저장하고 snappy로 압축이 되서 저장될 것.
    <br>

    - 파티션은 각 날짜에 대한 파티셔닝을 줘야한다. 그래야 나중에 조회할 때 이 기간에 대해서 조건을 주면 해당되는 날짜에 폴더에 있는 데이터만 읽게 된다. 그래서 sql에 퍼포먼스를 좀 높일 수 있는 조건으로 사용이 가능해진다.
    
    - 나중에는 이제 redshift에 spectrum이나 athena를 통해 조회할 때도 파티셔닝을 주게 되면 거기에 해당되는 테이터만 읽어오기 때문에 조회할때 금액적으로 저렴하게 조회할 수 있는 조건이 될 것이다.

    ![5-3-6](https://user-images.githubusercontent.com/86764734/157053126-88e9825a-d24a-47f1-97c1-689d6abe6f65.png)

    이렇게 해도 되고 athena에서 테이블 생성해도 된다.

5. Glue에 가서 확인 
    
    테이블 생성 확인

![5-3-7](https://user-images.githubusercontent.com/86764734/157053279-491febb6-de21-4c6c-aeb7-cecdf6e97237.png)

클릭해서 보면 정보를 볼 수 있다.

- 나중에 메타정보를 관리하게 되면 대부분 설명의 한글명을 좀 넣어주는 것이 관리하기에 좋다. 차후에 테이블을 생성할때 스크립에 한글명을 꼭 넣어서 테이블을 쉽게 파악할 수 있도록 하자!!

![5-3-8](https://user-images.githubusercontent.com/86764734/157053464-7aa891d7-4055-45a6-9ece-3ae63c8a4b89.png)

6. 우리가 위해서 쿼리로 어떤 데이터를 쌓을 지 확인을 했다. 여기서 주의할 점은 base_data가 파티셔닝으로 정의되있다. 파티셔닝으로 해당되는 부분을 맨 뒤에다 넣어주면 된다. 파티션에 해당되는 것은 항상 뒤로!

- 보면 파티션에 해당하는 base_date가 맨 뒤에 작성된것을 볼 수 있다.

![5-3-10](https://user-images.githubusercontent.com/86764734/157053836-c852075c-340a-4da3-9a5c-6fc098961842.png)

드래그 된 부분은 위에서 카피해서 붙여넣은것

sql은 줄을 맞추는 것이 가독성이 좋다! 그래서 꼭 맞춰줘라

이렇게 하고 실행을 하면 앞에서 만들어준 테이블에 데이터가 들어가게 된다.

![5-3-11](https://user-images.githubusercontent.com/86764734/157054014-66758aea-606c-4b99-bd47-60efac8e0aab.png)

7. 들어가는 데이터는 ‘매물별 사용자 조회수' 이벤트 로그에서 좀 정제해서 우리가 분석할 수 있는 환경으로 데이터를 넣어주는 작업을 했다. 

8. 데이터 확인 작업
    
    - 테이블에 해당되는 데이터를 셀렉해보자. 

![5-3-12](https://user-images.githubusercontent.com/86764734/157054149-24c8c0fc-6b36-41ce-8c54-2d0c5f088e1e.png)

---

이제 s3에 데이터를 넣고 emr을 띄워서 sql을 통해서 분석할 수 있는 환경을 만들어보았다. 또 아테나를 통해서 분석을 할 수도 있고 나중에 마트성으로 레드시프트를 운영한다면 레드시프트 스펙트럼을 통해 딥다이브 할 때 분석이 가능하다. 