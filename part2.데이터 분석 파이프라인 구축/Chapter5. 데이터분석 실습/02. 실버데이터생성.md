## 실버데이터 생성

---

aws콜솔로 이동해서 실습 진행하도록 한다.

1. 서비스 서치엔진에서 emr들어가서 클러스터 하나 띄우기
    
    - 될 수 있으면 우리가 실습할 때 스텝에서 스크립을 실행한 것들이 있기 때문에 복제해서 진행한다.

    - 복제 완료되면 이전 클릭해서 처음 단계인 소프트웨어 구성으로 이동하자!

![5-2-1](https://user-images.githubusercontent.com/86764734/156108642-59775270-c6b8-42f1-86a5-e1254db9a181.png)

2. 모든단계(선택사항)
    
    우리가 스텝단계에서 실행했던 스크립들이 등록되어 있다. 
        
    - 첫번째는 galglia
        
    - 두번째는 mysql드라이버 설치하는

![5-2-2](https://user-images.githubusercontent.com/86764734/156108899-762505ab-2ae8-49f5-adb4-759c83625de8.png)

- 에코시스템에 소프트웨어 설정이 되어 있다.

![5-2-3](https://user-images.githubusercontent.com/86764734/156108948-f300f5b7-9467-4e39-857d-fbb913036225.png)

- 중요한건 메타스토어 역할을 하는 데이터카탈로그를 활용을 할 것이기 때문에 해당부분 꼭 체크!

- 근데 외부 db에 메타스토어를 운영하는 경우도 있는데 관리포인트를 줄이기 위해 glue에 있는 카탈로그를 이용하는 것을 추천

![5-2-4](https://user-images.githubusercontent.com/86764734/156109001-852f3dc7-dab3-440f-a666-f55c5eed4502.png)

- 또 한가지 중요한 것은, 아테나 자체가 프레스토로 라는 에코시스템으로 구성되어 있는데

만약 프레스토를 사용하겠다고 체크를 하면 프레스토 자체도 메타스토어로 쓰겠다고 체크를 해줘야 프레스토에서 sql를 날릴 때 메타스토어를 참고해서 sql을 통해 분석을 진행할 수 있다.

![5-2-5](https://user-images.githubusercontent.com/86764734/156109102-06d0a78b-92ba-4965-910d-91ac3c501a77.png)

3. 노드와 인스턴스
    
    꼭 온디맨드 → 스팟 으로 변경하기!!!!!

![5-2-6](https://user-images.githubusercontent.com/86764734/156109209-d8fc93fe-f15a-4e2c-bbc5-63339bcf77ca.png)

- 확인 할 것은 초록색이 무엇인지 확인하고 서브넷 변경해주고 스팟인스턴스를 실행해줘여 된다. 

![5-2-7](https://user-images.githubusercontent.com/86764734/156109267-cd4aebd1-e681-450d-8aa0-43b02904ade9.png)

4. 클러스터 생성
    
    클러스터가 다 구성되고 나면 어플리케이션 이력에 가서 제플린 URL복사해서 들어가면 된다.

![5-2-8](https://user-images.githubusercontent.com/86764734/156109378-440a006d-3ad7-4b5c-8bfa-23bb6d84083f.png)

- 만약 위치가 변경된다면??

마스터보안에서 설정해주자. 대부분 에코시스템은 마스터에 있다.

![5-2-9](https://user-images.githubusercontent.com/86764734/156109799-ddb93c2f-f33e-4f9c-b520-b72950e31e0f.png)

- 인바운드 큐칙을 확인 후 없으면 인바운드 규칙편집

    - 모든 TCP/ 사용자IP/ 설명-my office

5. 어플리케이션 이력에서 zeppelin 카피해서 실행.

![5-2-10](https://user-images.githubusercontent.com/86764734/156110005-44f5e9d6-70d7-41c8-a6eb-1d8868c5a492.png)

6. 제프린 열기
    
    지금 미리 파일 불러와 있어서 확인되는 것. 우리는 import note에서 파일을 불러와야 한다.

![5-2-11](https://user-images.githubusercontent.com/86764734/156110163-4aead17c-85e7-48dd-be79-a897177dffdc.png)

### 단지별 사용자 조회수

1. 데이터 수집 관련된 아키텍쳐를 했을 때, 수집된 이벤트 로그가 S3에 있는 logs라는 폴더에 쌓일 것이라고 가정한다. 이것은 하루치의 데이터 이며 용량은 1.5기가 정도 된다. 

![5-2-12](https://user-images.githubusercontent.com/86764734/156111030-1923defb-a097-429f-9dbe-e5cb0a719d74.png)

2. 데이터 용량확인
    압축이 되어 있는 상태에서 용량이 1.5GB인 것을 확인할 수 있다. 약 10기가정도 되지 않을까 싶다. 이 데이터는 **브론즈 데이터**!

![5-2-13](https://user-images.githubusercontent.com/86764734/156111185-0e708569-782f-4300-8cf2-88ca8e4af900.png)

![5-2-14](https://user-images.githubusercontent.com/86764734/156111478-d6230534-b3d8-494a-b8cf-4474f575333b.png)

3. 지난번과 다르게 추가된 부분이 있을 것.
    
    이거는 우리가 hive테이블을 선언을 한 후에 여기에 데이터를 인서트해서 데이터를 넣을 것이라서 거기에 필요한 옵션 두가지.
    
    dynamic.partition -> 여러 폴더가 있으면 각각 나눠서 데이터를 분리해서 넣을 수 있는 옵션!

![5-2-15](https://user-images.githubusercontent.com/86764734/156111595-0e4d0db9-b203-4e87-b9f8-cb8f79b44d44.png)

4. 각각 라인을 파싱하는 소스코드
    
    이걸 통해 로그파일을 확인을 하고 기능을 짜야한다. 근데 이제는 데이터를 쌓을때 대부분 파일의 크기가 좀 커도 json 형태로 저장을 하는게 가장 유연하게 로그관리를 할 수 있다. 
    
    그렇지 않rh static하게 되면 배포할 때 변경작업이나 수정작업을 반복적으로 해야하기 때문에 그런것 없이 저장하는건 json 형태가 가장 좋다.
    

![5-2-16](https://user-images.githubusercontent.com/86764734/156111803-bbe7a799-0f5c-44aa-8ec9-5fc31c0d74cd.png)

5. **sqlcontext.cachetable**
    
    → glanglia를 띄어 보면 캐시영역이 잇는데 이 캐시영역에 데이터를 올릴 때 캐시로 올리는 옵션
    
    이 캐시 주는 부분은 데이터프레임형태를 주는 경우가 있고 이거는 logs라는 테이블을 tableview를 하나 만들어서 캐시를 주는 형태 근데 캐시된 테이블과 안된 테이블 두개가 있을 경우에는 대부분 캐시가 안된 것 기준으로 되기 때문에 두개의 테이블이 있을 땐, 두개나 캐싱을한 후에 sql을 날려야 한다.

![5-2-17](https://user-images.githubusercontent.com/86764734/156111986-0d8de45b-7a6e-41fb-83e0-5e8644cb39f1.png)

```
val spark: SparkSession = 
            SparkSession
            .bullder()
            .appName("StatsAnalyzer")
            .enableHiveSupport()
            .config("hive.exec.dynamic.partiton", "true")
            .config("hive.exec.dynamic.partiton.mode", "nonstrict")
            .gettOrCreate()
```

![5-2-18](https://user-images.githubusercontent.com/86764734/156112102-14e68356-6b70-4910-95b7-d21ff1a75f74.png)

6. 데이터프레임에 대한 데이터확인 

```java
logsDRAll.show
```

이렇게 캐싱을 하면 s3데이터를 다 캐시로 로딩을 해야한다. 그래서 전체에 대한 카운팅을 해줘야 모든 데이터가 올라간다.  

그래야 그 다음부터 좀 빠르게 분석을 할 수가 있다. 해당 셀을 실행을 하고 ganglia를 통해 모니터링을 계속 해주자! 

캐시자체자 조금씩 올라가는 것을 볼 수 있다. 카운팅을 한번 해주면 테이블에 캐싱이 될 것이고 얘가 마스터성 테이블이면 배치돌 때 얘를 한번 카운트 all을 돌리고 그것을 기준으로 해서 나머지 밑에 쿼리들이 캐시에 대해서 적용받도록 플로우를 짰다. 

- 카운팅

```
%sql

select count(*)
from logs
```

8. 데이터 보기
    
    우리가 아파트에 대한 단지조회에 대해 저장을 진행할 것. 다른 테이블과 조인해서 이 데이터를 풀게 아니고 우리가 실버데이터를 만들것이기 때문에  카탈로그가 아파트인 것만 불러온다. 

```
%sql

select *
from logs
where item_category = '아파트'
```

9. 아파트상세를 여러 각도로 분석
    
    다양한 조건을 쿼리문을 통해 확인해보자.
    
    옆으로 넘겨서 다양한 컬럼을 확인하자
    
    이렇게 logs를 테이블뷰를 통해 확인

```
%sql

select *
from logs
```

11. 사용자에 대한 아파트단지 조회
    
    그다음에 우리가 정의할게 사용자에 대한 아파트 단지 조회에 대한 것이니까 그런 조건을 넣어주었다.

   zeppelin에서 쉽게 할 수 있는게 시각화를 들 수 있다. 아래처럼 바차트로 설정해서 보면 좀 더 시각적으로 이벤트의 흐름을 알 수가 있다. 

![5-2-19](https://user-images.githubusercontent.com/86764734/156116796-2bce99fb-41bd-4173-b439-a7b888f1b1ed.png)

12. 우리가 목적하는 바는 아파트단지조회!
    
    우리가 쌓는 방법은 기준일자별로 사용자가 이 단지를 언제 조회 했는지 로그데이타 기반으로 정리해서 저장을 할 것이다.  4개의 컬럼만 뽑아서 저장!

### parquet 형태 저장