## AWS Crawler 실습

---

우리가 지난시간에 여러 데이터 형태를 변경하는 실습을 진행을 했는데

거기에 쌓여있는 데이터를 크롤링을 통해서  glue에 있는 데이터 카탈로그에 등록하는 것을 실습진행.

그렇게 쌓인 것을 분석을 해야하는데 athena 서비스를 통해 조회하는 것 까지 진행!

## glue crawler

---

### crawler 실행

---

1. aws콜솔창에 glue검색하여 접속

![4-3-1](https://user-images.githubusercontent.com/86764734/155884083-c7fcfc26-5b1c-445d-b180-5a750924e3dd.png)

glue에 가면 맨 왼쪽 상단에 데이터베이스를 볼 수있다. 
크게 dbms도 스키마, 큰 단위의 데이터베이스를 생성하거나 아니면 스키마를 생성하게 되어 있다. 

그래서 우선 실습을 위해 데이터베이스를 하나 생성하도록 하겠다.

![4-3-2](https://user-images.githubusercontent.com/86764734/155884106-627ccd04-07b8-4ba5-985b-21c69a0edd82.png)

2. 데이터베이스 클릭하여 데이터베이스 추가

![4-3-3](https://user-images.githubusercontent.com/86764734/155884133-f077c14b-3ab3-4396-9991-3b53a4bf2c57.png)

![4-3-4](https://user-images.githubusercontent.com/86764734/155884142-3dd36eb9-38f0-4280-b4f8-3bf38a618739.png)

3. class라는 테스트용 데이터베이스 클릭해서 들어가보자
    
- 클릭해서 들어가보면 테이블이 하나도 없다!
    
    - **크롤러를 사용하여 테이블 추가**라는 메뉴가 보인다.
    
    - 아니면 좌측에 **크롤러**라는 메뉴가 있다.

![4-3-5](https://user-images.githubusercontent.com/86764734/155884168-d7719487-2dd6-4ec5-bff3-d2783c096f85.png)

![4-3-6](https://user-images.githubusercontent.com/86764734/155884194-0e0fa985-1350-402d-add6-43ce940f1048.png)


### crawler 추가 

---

1. 좌측에 있는 크롤러를 클릭해서 들어가보면 해당 화면 확인 가능하고 크롤러 추가 클릭

![4-3-7](https://user-images.githubusercontent.com/86764734/155884224-bdbd141a-01b7-44af-805d-a35af797abeb.png)

2. 크롤러 추가
    
- 크롤러 이름 class-crawler

![4-3-8](https://user-images.githubusercontent.com/86764734/155884245-a038f8c1-63a6-444c-b524-5ebda404cd17.png)

![4-3-9](https://user-images.githubusercontent.com/86764734/155884265-2c66e930-82c4-4ab8-907c-ea0249715207.png)

3. 다음 누르면
    
- crawler source type → 크롤러 데이터 소스타입을 어디서 할건지. 우린 s3에 할거라 그대로 
    
- repeat crawls of s3 data stores →

![4-3-10](https://user-images.githubusercontent.com/86764734/155884299-1490d153-ff9a-4620-b208-24baf90dd751.png)

4. 데이터스토어추가
    
- 예전에는 S3만 있었는데 클릭해보면 여러가지 확인 해 볼 수 있다. 이제

![4-3-11](https://user-images.githubusercontent.com/86764734/155884321-df0c10c7-983b-464a-8e81-099c1c3c2ef3.png)

- 우린 S3선택!

![4-3-12](https://user-images.githubusercontent.com/86764734/155884341-30cc7563-f5b8-4b7f-8149-27ef4f7e735a.png)

5. 다음위치의 데이터를 크롤링

-  다른사람이 만들어 놓은 S3 버킷의 경로로도 가능!

![4-3-13](https://user-images.githubusercontent.com/86764734/155884366-0dc3522d-118b-4236-85f4-6c78b019d850.png)

![4-3-14](https://user-images.githubusercontent.com/86764734/155884392-7e5fa2b0-52dd-4d6f-ad8b-9474a86684e7.png)

6. 경로선택
    
- fc-class/ods/test/
    
- test밑에 우리가 여러가지 컨버팅 진행한 파일들이 있다. 

- test폴더에 있는 이 s3파일들을 하나의 테이블로 크롤링 해 달라!이런 뜻

![4-3-15](https://user-images.githubusercontent.com/86764734/155884428-14348eb6-86ba-4edb-b3ff-509fd6bfe2d8.png)

![4-3-16](https://user-images.githubusercontent.com/86764734/155884451-cfe95557-ac82-4bd9-ae3a-55d12adc5e0b.png)

7. 다른 데이터 스토어 추가
    
- 더 여러개를 한번에 할 수도 있는 설정.

    - 우린 아니요 선택하고 다음
    
    - 오른쪽에 보면 선택된 데이터 스토어 확인!

![4-3-17](https://user-images.githubusercontent.com/86764734/155884504-1d877032-b265-45d9-bb29-2844a33aac5d.png)

8. iam 역할 선택
    
우리가 s3라는 서비스에 있는 것을 glue에 있는 크롤러를 통해 가져오는 것이기 때문에 거기에 따른 권한을 생성해줘야 한다. 

    우리가 firehose를 통해 s3에 저장할 때도 iam역할을 신규로 생성했던 것을 기억할 것이다.

    기존꺼를 선택할건지 새로 만들기 옵션이 있는데, 실습을 위해 iam역할 생성으로 선택하고, 해당된 크롤러 클래스라는 역할을 새로 생성!
    
    근데 너무 많이 새로 생성하면 나중에 iam에 가보면 여러개가 생성되어 있기 때문에 될 수 있으면 기존에 있는 것을 사용하는 것이 좋다!

![4-3-18](https://user-images.githubusercontent.com/86764734/155884528-7e3f82b9-17e1-45ee-a69c-c296f57d3457.png)

![4-3-19](https://user-images.githubusercontent.com/86764734/155884544-4f9a7830-d46c-4f39-832f-a4caa7df1e2c.png)

9. 크롤러의 일정 생성
    
- 빈도 → 얘가 특정 s3폴더에 주기적으로 올라오는 것이면 시간별등등 선택할 수 잇다.
    
    - 이 크롤러를 배치를 돌릴 수 있는 옵션을 가져갈 수 있다. 
    
    - 우린 온디맨드실행으로 한다.

![4-3-20](https://user-images.githubusercontent.com/86764734/155884569-c0cda5a5-379d-43cd-ad30-924975bb2e62.png)

![4-3-21](https://user-images.githubusercontent.com/86764734/155884591-0d4530b4-b73c-4b87-8539-2e7d7d54a72b.png)

10. 크롤러의 출력 구성
    
- 데이터 베이스: 우리가 앞에서 하나 생성을 했으니까 그것을 선택하주면 된다!
    
- 테이블에 추가되는 접두사(선택항목): 테이블을 서로 구분하기 위해서 접두어를 쓸 수 있는데 스킵!

![4-3-22](https://user-images.githubusercontent.com/86764734/155884619-b34ec7a6-2972-479f-a36a-2f3fb1d08378.png)

![4-3-23](https://user-images.githubusercontent.com/86764734/155884636-65f92078-a550-4d5e-b587-108c9ad5a6c5.png)

11. 모든 단계 검토
    
    하나의 크롤러가 생성된 것을 컴토할 수 있다. 
    
    크롤러 정보를 확인 
    
    데이터 스토어는 저장되어 있는 타겟정보를 소스정보를 볼 수 있고,IAM 역에서 거기에 따른 iam룰 정보가 있다.
    
    일정과 출력에 대한 정보도 확인 할 수 있다. 
    

![4-3-24](https://user-images.githubusercontent.com/86764734/155884664-6425b239-86c6-41f9-aa5a-bae36ae3b000.png)

12. 마침을 클릭하면 크롤러 하나가 생성된다!
    
    예전에는 이제 바로 실행할 건지 물어봤는데 없어진거 같네용
    
    클릭을하고 크롤러 실행을 눌러바

![4-3-25](https://user-images.githubusercontent.com/86764734/155884703-847fcbef-04cf-4857-9cf8-26dcc6b13c5e.png)

13. 크롤러 실행
    
    s3폴더에 가서 파일을 다 읽은 다음에 유형이 어떤 타입인지 그것을 판별을 하고,
    
    우리가 spark로 데이터 프레임을 만들었던것 처럼 전체를 읽은 다음에 각 컬럼에 어떤 값이 있는지를 확인.
    
    json이나 파케이 같은 경우는 컬럼정보가 쉽게 들어갈 것이다.


![4-3-26](https://user-images.githubusercontent.com/86764734/155884725-2cb50e2e-142f-4ea9-8b3e-f0713555ef3e.png)

14. 한번 s3에 가서 확인해보자
    
    현재 4개의 폴더가 있는 것으로 확인 된다. 그래서 요 파일! 똑같은 것이지만 어떻게 읽히는지 확인을 해보자

![4-3-27](https://user-images.githubusercontent.com/86764734/155884753-16ab8999-1327-4e14-be53-699c2b35571f.png)

15. 크롤러 실행완료
    
    지금 테이블이 3개 추가된 것을 확인할 수 있다. 결과적으로 그 테이블이 어떻게 생성이 됐는지 한번 확인해보자.

![4-3-28](https://user-images.githubusercontent.com/86764734/155884783-9371ed49-136d-43c3-a688-8bf2b83b1a59.png)

16. 왼쪽 메뉴탭에서 테이블로 이동!
    
    parquet같은 경우는 메타값이 아예 parquet 파일 구조로 판단하기 보다는 데이터에 대한 데이터 타입까지 가지고 있다. 
    
    그 값을 읽어서 그대로 진행을 하기 때문에 좀 더 빠르게 읽었을 것. 
    
    분류 → 어떤 타입인지! 
    
    입출력 형식 → 얘를 컨버팅하는데 쓴 입출력 형식이 있다. 
    


![4-3-29](https://user-images.githubusercontent.com/86764734/155884813-2619771c-1ccd-480e-9b53-2bb9f5025a5c.png)

![4-3-30](https://user-images.githubusercontent.com/86764734/155884834-3efc84fc-3d97-4c33-855a-912d5e0c6862.png)

![4-3-31](https://user-images.githubusercontent.com/86764734/155884865-10c47ad3-ff19-4931-8cec-94cafd6fcf25.png)

17. 오른쪽에 보면 버전 확인 가능
    
    데이블이 만약 변경이 되면 이 테이블에 대한 버전관리가 될 것.
    
    스키마를 또 편집할 수 있다.

![4-3-32](https://user-images.githubusercontent.com/86764734/155884896-acc917f7-84e9-4311-87f6-32acafbf33ce.png)

18. 스키마편집
    
    열추가도 가능 
    
    근데 조심해야한다 기존에 있는 메타값이 깨지면 나중에 분석할 때, 아테나를 통해서 분석을 할건데 그럴때 데이터를 다 못 읽어 올 수도 있다.

![4-3-33](https://user-images.githubusercontent.com/86764734/155884935-4de3b6bd-0bf6-4724-9ec9-0a884652bdae.png)

![4-3-34](https://user-images.githubusercontent.com/86764734/155884953-7d44ecbe-6879-4216-b103-b5605c547c61.png)

19. json을 열어서 봐보자!
    
    컬럼이름이 쭉 다 되어 있다. 왜냐면 키밸류이기 때문에 값을 읽을 때, 이 값에 컬럼에 대한 밸류값이 정확하게  정해져 있기 때문에 쉽게 판별을해서 얘가 메타값을 구성한다. 
    
    입출력 형식만 좀 다른 형식인걸 볼 수 있다.
    
    속성보기 → json형태인 것 확인 가능 
    
    스키마편집 → 똑같이 스키마 편집 가능 
    
    여기서 테이블 삭제도 가능

![4-3-35](https://user-images.githubusercontent.com/86764734/155884978-a6784d00-19dc-480d-9093-619771a5b959.png)

![4-3-36](https://user-images.githubusercontent.com/86764734/155884994-f9634eb9-e90e-4447-96d6-5719c872c3b8.png)

20. 테이블에 대한 편집도 가능
    
    될수 있으면 안 건들이는게 좋다
    
    스키마 값이 깨져서 다시 크롤러를 돌리는 번거로움을...

![4-3-37](https://user-images.githubusercontent.com/86764734/155885017-8588613a-333e-4f6b-8399-61702bf05123.png)

그리고 주의할게 우리가 나중에 파티셔닝을 배우면 이 파티션 값도 저장이 되어 있다. 이 테이더 스토어에

그런데 그 파티션값이 업데이트가 안되면 여기서 조회했을때 우리가 테이더를 볼수가 없다.

21. csv파일도 확인
    
    헤더값을 읽어서 컬럼을 구성을 해야하는데 옵션을 넣어주지 않아 그냥 컬럼을 넘버링이되서 컬럼이 구성되었다. 

![4-3-38](https://user-images.githubusercontent.com/86764734/155885046-79b64a89-20be-4af2-bd98-839e1c42d6f6.png)

그리고 우리가 크롤링을 통해서 데이터를 등록을 했으니까 그것을 아테나 통해서 분석해보자. 

### Athena

아테나란?
    
아테나는 서버리스이고 비용은 분석을 위해서 우리가 sql을 쓰면 거기에 따른 데이타 로딩 양이 있다. 그 로딩양에 따라서 차징이 되는 서버리스 서비스이다. 
    
아테나의 엔진은 프레스토를 사용했고, 최근에는 아테나가 기본적인 ml을 수용할 수 있게 쿼리로 할 수 있게 업그레이드 되어있다. 
    
그리고 처음 접속하면 첫번째 쿼리를 실행하기 전에 s3에 쿼리 결과를 저장할 수 있는 공간의 위치를 설정해야 한다고 나온다.

---

1. athena열어주기

![4-3-39](https://user-images.githubusercontent.com/86764734/155885075-35f05c67-aea3-4e92-a516-2cefa79908ea.png)

2. 쿼리편집기
    
- 보기설정 눌러서 이동

![4-3-40](https://user-images.githubusercontent.com/86764734/155885094-e5a952a0-cac2-43bb-95e8-a25d5283f7ab.png)

3. 관리 클릭

![4-3-41](https://user-images.githubusercontent.com/86764734/155885126-9cfd6e1e-da78-4200-98e0-94f43f537f18.png)

4. s3찾아보기 → 폴더를 하나 선택을 해주자

![4-3-42](https://user-images.githubusercontent.com/86764734/155885144-8178bf5e-4221-4df1-bd87-3ad5c712696a.png)


![4-3-43](https://user-images.githubusercontent.com/86764734/155885164-9e2828ae-05e0-4b35-a5ab-8362e68a564d.png)

5. 선택하고 저장

![4-3-44](https://user-images.githubusercontent.com/86764734/155885265-a5e63843-1579-47fb-8a8e-ec6e747e8fbd.png)

- 해당 위치에 설정이 된다. 

![4-3-45](https://user-images.githubusercontent.com/86764734/155885298-025fbb1e-ef38-47fe-8f83-1c7028b917a5.png)

6. 다시 아테나 쿼리편집기로 이동

보면 데이터 원본 → 데이터 카탈로그,

데이터베이스 → 아까 우리가 만들어놓은거 

테이블 → 클래스 안에 있는 테이블 3개가 확인될 것!

![4-3-46](https://user-images.githubusercontent.com/86764734/155885317-6e218d71-91da-4491-ba56-86c33445f49f.png)

7. parquet먼저!
    
    테이블 미리보기 → 10개의 데이타를 볼 수 있는 쿼리문 실행 어떠

![4-3-47](https://user-images.githubusercontent.com/86764734/155885334-845f9b2f-59b0-4d12-a506-2fb0a0706ca5.png)

```sql
SELECT * FROM "class"."danji_parquet" limit 10;
```
어떤 컬럼으로(속성으로) 구성되어 잇는지 확인 가능 


그래서 이렇게 분석을 할 수가 있다. 파일이 아무리 커도 아테나가 s3에 저장되어 있는 것을 읽어서 분석할 수 있는 환경을 제공해준다. 

우리가 좀 더 익숙해지고 데이터 저장방식들을 이해하게 되면 나중에 저장을 할 때, 파티션을 적어도 날짜단위의 파티셔닝을 꼭 해야한다. 


읽은 만큼의 비용을 제불해야하기 때문에 일부를 볼 수 있도록  limit나 이런 함수들을 써서 일부 데이터를 확인 할 수 있는 명령어를 알아야한다!!
