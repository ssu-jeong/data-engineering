## zeppelin을 통한 분석 - mysql

---

앞서 rds를 하나 실행을 햇다.

우리가 emr의 spark를 활용을 해서 데이터를 어떻게 mysql에 저장을 하는지 실습진행

#### EMR 실행

- 이전에 실행했던 것 복제

![3-5-1](https://user-images.githubusercontent.com/86764734/155149041-13806b35-af70-4b9f-8b60-781ce4fe728f.png)

- 이전에 강글리아 띄우기 위해 띄웠던 스텝을 그대로 가져갈 수 있다. 

![3-5-2](https://user-images.githubusercontent.com/86764734/155149257-97749979-e5b4-4ac1-9178-2c9c97fbbfae.png)

#### zeppelin 실행

- 어플리케이션 이력으로 가서 zeppelin URL을 가져와서 새탭에서 열어준다.

![3-5-3](https://user-images.githubusercontent.com/86764734/155149430-4c2b66af-a7ad-4e1d-b2c7-781b4d0a24be.png)

- import note를 통해 제공받은 소스코드 열어준다 → 3.3 RDS read write-scala.json

![3-5-4](https://user-images.githubusercontent.com/86764734/155149641-9216da4d-f333-49e8-aebd-18d790a6e137.png)

- 소스코드 올라온 것 확인하고 클릭해준다.

![3-5-5](https://user-images.githubusercontent.com/86764734/155149749-dd6dc58e-a32b-40ba-a91b-4cf9e5b1c881.png)


- 1.초기화 

  - → 디비 접속을 위한 패키지를 import

![3-5-6](https://user-images.githubusercontent.com/86764734/155149829-0b1f06d5-7adf-47b3-aaf0-1b3037cca50e.png)

- 2. RDS Read 

  - → 접속할 수 있는 정보들

**JDBC**
(Java Database Connectivity)는 자바에서 데이터베이스에 접속할 수 있도록 하는 자바 API이다. JDBC는 데이터베이스에서 자료를 쿼리하거나 업데이트하는 방법을 제공한다.

---


모든 DB에 spark가 접속하기 위해서는 jdbc드라이버를 꼭 설치해줘야한다. 

기본적으로 mysql드라이버나 , aws서비스인 레드시프트 접속드라이버가 디폴트로 설치가 되어있지 않기 때문에 직접설치를 해줘야한다. 

과거에는 zeppelin이 그런 라이브러리 레포지토리를 따로 관리해서 설치를 쉽게 지원을 해주긴 했었다. 

메인의 기능 그대로 가져가서 설치를 쉽게 할 수 있게 제공을 했었는데, 현재는 막혀서 우리가 자료에 제공된 부분인 실직적으로 스크립을 통해 스파크의 라이브러리에 직접적으로 mysql에 드라이버를 다운로드 받아서 설치하는 방법으로 진행해야한다. 

지난번 강글리아에서 실습했던 것 처럼 오픈포트 하듯이 픽스해서 올려주면 바로 드라이버 설치!!


#### 드라이버설치
    
- 라이브러리를 wget으로 다운받아서 usr/lib/spark/jars 에 넣어주면 된다.

![3-5-7](https://user-images.githubusercontent.com/86764734/155335277-1e4324c6-fa3e-485e-9271-dc0528d1cc16.png)

![3-5-8](https://user-images.githubusercontent.com/86764734/155335413-116befb5-6aa8-46cc-b42f-026decaa7712.png)

- 제플린같은 경우 세션이 접속되었을 때는 저기에 라이브러리를 설치를 해도 바로 적용이 안되기 때문에 제플린과 스파크가 접속된 그 세션을 재기동!

- 일단 스크립은 S3에 있기 때문에 새탭에서 콘솔창을 열어서 S3로 이동
    
    - 지난번처럼 S3 url 복사해서 EMR에 가서 단계로 이동

![3-5-9](https://user-images.githubusercontent.com/86764734/155335549-86699fd2-c612-4b5a-99c0-9286fbdcff97.png)

- 단계로 이동

![3-5-10](https://user-images.githubusercontent.com/86764734/155335725-381d30ca-bf88-41c2-8e6e-24b333b1e516.png)

- 쉽게하는 방법은 단계를 클릭하고 ‘단계복제'클릭

![3-5-11](https://user-images.githubusercontent.com/86764734/155335841-4347dd1e-a866-4dc8-9c0b-9aa300fdaf47.png)

- 인수를 아까 복사한 url로 변경해준다. 그러면 자동적으로 스크립이 실행!
    
- 스크립이 실행이 되면 mysql에 접속할 수 있는 jdbc드라이버를 스파크에 jars에 카피하는 스크립까지 진행. 
    
  - 다 진행이 되면 제플린 에센셜을 재시작해줘야한다!

![3-5-12](https://user-images.githubusercontent.com/86764734/155336336-e444214a-8f52-49ae-a537-a19ac1c03974.png)

- 만약 실행에 에러가 나면 로그보기를 통해 어떤 에러가 나는지 확인할 수 있다. 
    
    - 설치 완료

![3-5-13](https://user-images.githubusercontent.com/86764734/155336506-bd50e6fd-7b0c-4de6-a0be-7976fd0fa23d.png)

#### zeppelin 재시작

이제 아까 켜두었던 제플린으로 가서 재시작하자!

세션에 대한 리스타트는 얘가 인터프리터이기 때문에 접속정보는 오른쪽 위에 aunonymous를 클릭해서 인터프리터에 접속 

![3-5-14](https://user-images.githubusercontent.com/86764734/155336651-9b6424da-51fe-4334-b8cc-fd6072af08f7.png)

- spark로 이동 
    
    - spark를 재기동해주면 된다! 그러면 현재 jdbc 드라이버를 설치한 부분이 이 세션에 적용이 된다. 
    
    - 다시 홈을 눌러서 해당 노트로 이동!

![3-5-15](https://user-images.githubusercontent.com/86764734/155336927-80007f70-bc3d-47e9-bb0f-e05fb54314f0.png)

![3-5-16](https://user-images.githubusercontent.com/86764734/155337026-ae068402-6a53-42aa-8e98-35965912b9bb.png)

#### RDS read write

13. 위에 부터 하나씩 실행하면 됨!
    
    아까 카피해두었던 엔드포인트 두번째 코드에 입력하고 실행
    
    유저
    
    패스워드 기입

![3-5-17](https://user-images.githubusercontent.com/86764734/155337149-1e53f727-2a2c-4f29-b6b2-26924171cf20.png)

우리는 dw에 저장을 할 것!

접속정보를 properties를 통해 설정을 해준다

![3-5-18](https://user-images.githubusercontent.com/86764734/155337350-9da8fa80-7c50-4a4b-b3f1-aed7fc4a3eed.png)

설정해준뒤 실행하면 설정된 것 확인가능!

![3-5-19](https://user-images.githubusercontent.com/86764734/155337444-593ba5b1-dcd9-4f4d-9b8d-76ac28fb6f81.png)

14. 그다음 우리가 db에다 쌓을 데이터를 로딩해야한다!!!
    
    똑같이 우리가 가지고 있는 csv파일을 그대로 db로 로딩할 것!
    
    우리가 지금 하는 것은 여러 채널의데이터를 읽고 그 채널의 데이터를 쓰는 실습이니까 여러 방법으로 실습을 진행하도록 할 것!
    
    그래서 S3에 이쓴ㄴ ods폴더로 이동해서 danji_master.csv 파일 클릭
    
    다른걸로 진행해도됨 우리가 지난시간에 json도 만들고 파케이도 만들었듯이 그 데이터를 읽어서 써도 된다.

![3-5-20](https://user-images.githubusercontent.com/86764734/155337612-73cd50bf-fd2a-4eae-9279-823cea4a7e8f.png)

15. csv를 읽어서 db에 접속하기 위해서 데이터 프레임을 만들자 
    
    → 그 만든 데이터 프레임을 바로 사용할 수도 있고

![3-5-21](https://user-images.githubusercontent.com/86764734/155337786-76e3f9fc-4a54-4138-8a7a-aeed0a4b5a0a.png)

가설을 세우는 것! 파일이 단지마스터라는 파일이 큰것을 읽어서 가공을 한다.

우리가 될 수 있으면 의미를 많이 전달할 수 있는 용어를 사용하자 

단지에 대한 마스터성 테이블이다!

![3-5-22](https://user-images.githubusercontent.com/86764734/155337932-940172ef-f31e-489c-91b4-c577514347dc.png)

우리가 사용할 컬럼을 선택해 주었는데 처음엔 전체를 보고 db를 통해 분석할 때 필요한 것들을 골라서 분석을 하고 그 분석된 쿼리를 그대로 db에 적용

![3-5-23](https://user-images.githubusercontent.com/86764734/155338106-1f14aed5-838a-4cc5-ac02-caa4e48b0644.png)

16. 그리고 그것을 db에 써보자~
    
    spark.sql을 통해 sql을 실행을 하면 하나의 데이터 프레임이 생성된다. (데이터프레임 → 엑섹시트와 같은 하나의 시트) 이 데이터 프레임을 그대로 하나의 테이블로 만들면 된다!
    
    데이블 이름을 ‘danji_master’로 설정
    
    그다음 타겟을 dbms에다가 mysql을 만들었으니 거기에 접속해서 아래의 sql 데이터프레임을 그대로 쓸 수 있다. 
    
    그래서 형식은 똑같다.  → write를 하는데 mode는 ‘append’( append는 추가!) 그리고 overwrite하면 기존에 데이블이 있다면 기존거를 삭제하고 새로운걸 쓴다! dbms같은 경우는 기존에 테이블을 삭제하고 새로운 테이블을 다시 만든다. 이 점 유의해서 사용하자!!
    
    jdbc를 통해서 url, table 명으로 실행을하면 된다.
    
    위에서 돌렸던 쿼리문을 그대로 가져와서 돌려야 에러가 안난다!!
    
    이 부분을 실행을하면 db에 접속해서 쿼리뭄ㄴ에 해당되는 컬럼내용을 가져다 테이블로 쓸 수 있게 된다. 
    
    실행하고 시간이 오래 걸리네요... 지금 우리는 db접속 자체를 현재 열어놓은 게 내 ip에서 접근가능하도록 열어두었기 떄문에, 아마 스파크에서 mysql 쪽에 포트가 오픈되어 있지 않았기 때문에 접속에러가 날 것 같다....
    
    그래서 그 포트로가서 인바운드를 열어주도록 할 것!

![3-5-24](https://user-images.githubusercontent.com/86764734/155338224-c5bd78e3-065f-4025-a68d-707a1c83ecce.png)

17. spark가 sql에 접속할 수 있도록 포트를 열어주자 → 실습을 위해 그냥 anyone으로 열어주자
    1. 새 콘솔창에서 rds를 검색해서 들어가자

    ![3-5-25](https://user-images.githubusercontent.com/86764734/155338378-ca0bae7e-d487-4295-8b3b-7f266ed88fe7.png)

    2. 우리가 아까 실행했던 myclass 클릭해서 들어가면 
    3. 보안그룹규칙을 수정

    ![3-5-26](https://user-images.githubusercontent.com/86764734/155338522-f95dfd7c-a793-4f8c-9e89-64c108168e91.png)

    4. 인바운드 규칙에 가서 오른쪽 윗부분에 있는 인바운드 규칙편집

    ![3-5-27](https://user-images.githubusercontent.com/86764734/155338642-0a1300d6-9cda-4ad4-a74a-066a69e59ebd.png)

    5. 우리가 추가한 부분의 소스정보를 사용자 지정이 아닌 

    ![3-5-28](https://user-images.githubusercontent.com/86764734/155338781-a21d5dc6-cc6a-40be-883f-f8135468f5c3.png)

    6. anyone으로 수정
    
    안그러면 프라이빗 아이피 영역을 확인하고 해당 영역을 열어주어도 된다. 
    
    이렇게 되면 vpc 내부에 있는 프라이빗 아이피를 가지고 서로 통신을 하게 된다.

    ![3-5-29](https://user-images.githubusercontent.com/86764734/155338933-e6d1f962-afa3-4ad8-a336-98b431e112dc.png)

    7. 변경을 했으니 다시 제플린으로 가서 해당 쉘 실행!! 
    
    → 시큐리티그룹을 확인 한 후에 다시실행을 해주게 되면 정상적으로 dbms에 데이터가 써질 것이다.

    ![3-5-30](https://user-images.githubusercontent.com/86764734/155339087-c0b1ebbf-02b1-46b5-9928-aa03fdb643e1.png)

    8. 그럼 가서 확인을 해보자 쌤은 데이타그립을 사용하기 때문에 거기서 확인
    
    새로고침 해보면 테이블이 생성된 것을 확인 할 수 있다.

    ![3-5-31](https://user-images.githubusercontent.com/86764734/155339220-c68994dd-ae2b-4096-a71f-fae87ee76dfa.png)

    여기서 추가적으로 설명할 것은 

    spark에서 테이블을 쓸때는 실질적으로 얘가 인티값을 빼고 스트링에 해당되는 부분은 다 텍스트 타입으로 생성을 한다. 그렇게 되면 분석을 할때, 컬럼자체가 크기 때문에 mysql에서 sql자체가 느려질 수 있다. 이 부분은 컨버팅이 된 후에 데이터를 입력하고 ‘modify table’ 이나 이런 방법을 써서 이 컬럼에 맞는 사이즈로 변경을 해줘야 한다.

    ![3-5-32](https://user-images.githubusercontent.com/86764734/155339467-0e4996b4-4b1c-420c-9586-4a1f889ae1d4.png)

    9. 쌓인 데이터를 확인해보자 
    
    만약에 스파크를 통해서 여러가지 데이터가공을 한 다음에 mysql에 저장을 했다고 하면, 많은 데이터 중에 데이터를 줄이고 필요한 데이터만 mysql에 저장한 것이 될 것이다.

    ![3-5-33](https://user-images.githubusercontent.com/86764734/155339770-93624c9d-be05-4e1c-ad15-865562811174.png)

    18. 데이터를 써봤으니(write) 그 다음엔 읽는 부분(read)을 진행하보자 
    
    → 앨리야스를 꼭 줘야한다?? 안주면 에러가 난다 .
    
    우리가 앞에서 설정해주었던 것들을 모두 명시해주고 실행을 누르면
    
    db에서 쿼리를 통해 데이터프레임을 하나 만들었다.

    ![3-5-34](https://user-images.githubusercontent.com/86764734/155339889-34e7ef26-69da-4a7d-a012-aaed20ab29bc.png)

    밑에 보면 데이터프레임을 통해서 데이터가 db에 있는 정보를 읽어온 것이다. 약간 시간이 걸렸는데 그때 db에 가서 쿼리를 실행해서 이 정보를 가져다가 데이터프레임에 넣고 실행을 한다. 

    스파크 같은 경우에는 트랜스포메이션과 액션 func이 나눠져 있다. 그렇기 때문에 컴파일한다고 해서 바로바로 db 가져오는 것이 아니라 액션 func을 실행하고 가져오게 된다. 그러기 때문에 쇼라는 펑션을 실행을 했을 때 db에서 데이터를 가져다가 화면에 뿌려준 것

    ![3-5-35](https://user-images.githubusercontent.com/86764734/155340645-4a07eb2a-1a2e-44d9-b9bd-1dafc9e3b835.png)

19. 지금까지 여러 형태로 소스와 타겟을 바꿔가면서 실습을 진행했다.
20. 이것만 알아도 데이터를 가공하는 부분을 다른 영역이기 때문에 이 부분만 이해를 해서도 여러형태의 데이터를 읽고 가공해서 쓸수가 있다. 
21. 이 부분을 잘 이해를 하고 다음시간 부터 실질적으로 로그파일을 가지고 실버데이터를 만들고 골드데이터를 만드는 프로세스로 실습을 진행할 것이다.