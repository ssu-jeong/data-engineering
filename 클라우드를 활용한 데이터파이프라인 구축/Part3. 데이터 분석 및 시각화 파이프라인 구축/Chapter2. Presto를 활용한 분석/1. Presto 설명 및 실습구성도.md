## Presto 설명 및 실습구성도

---

### Presto 이용해 S3 저장되어 있는 Hive Table Data 를 BI툴에 제공하기

---

presto를 통해서 물리적으로 저장되어 있는 s3데이터를  어떻게 논리적으로 저장을 하는지 실습해보았다. ->hive 테이블로 저장

그래서 hive 테이블을 통해서 어떤 방법을 sql을 통해 분석을 하는지와 그 다음에 bi툴인 타블로를 통해 어떻게 분석을 할 지 실습


#### Presto란

![6-2-1](https://user-images.githubusercontent.com/86764734/158183804-82b71540-2c03-42bb-a483-ce7f271e4ff1.png)

페이스북에서 빅데이터에 대한 빠른 분석을 하기 위해 나온 쿼리엔진! presto 자체는 과거에 dbms처럼 특정한 레포지토리를 가지고 있진 않는다. 그래서 distribute 시퀄엔진이라고 보면 된다. 

논리적 저장소. 즉, 메타정보를 저장하고 있는 hive-metastore가 필요할 것이고 여러개 워커들이 존재할 것.

기존에는 hdfs에 데이터가 저장되어 있으면 그 데이터를 기반으로 sql엔진들이 데이터를 분석할 수 있게 지원을 해줬다. 근데 이제 우리는 s3로 변경해서 그런 부분을 분석할 수 있게 했다.

#### coordinator & worker

#### coordinator 

coordinator가 클러스터이기 때문에 마스토 노드와 워커노드라고 이해하면 된다. 

용어 자체가 코디네이터는 사용자가 sql을 짜서 실행시키면 sqlr구문을 분석을해서 sql에 대한 실행계획을 세운다.

쿼리플랜이 잡을 어떠한 워커들한테 분류를 해서 실행할 건지에 대해서 계획을 세우고 워커들한테 잡을 제공하는 그런 기능을 가지고 있고, 혹시 그 워커중에서 이상이 생겼을 때 잡을 재실행하는 부분에 대해서도 코디네이터가 관리한다. 

그리고 작업자가 쿼리를 수행했을때 그 결과를 작업자에게 전달해주는 역할!

#### worker

워커는 코디네이터가 준 작업을 실행하고 그 데이터를 처리한 다음에 통신은 restAPI를 통해서 코디네이터와 통신을 할 것이고 결과를 코디한테 전달하는 역할 워커가 많을 수록 속도가 빠를 것

![6-2-2](https://user-images.githubusercontent.com/86764734/158184123-d4b6b70b-9370-48eb-9bf3-516fdb5da4e2.png)

#### 구성

**커넥터**

가장 큰 분류 물리적으로 hive나 dbms에 접속하기 위한 커넥터 어뎁터를 말하는 것

카탈로그의 물리적 정의라고 보면 된다.

**카탈로그**

이 물리적 엔드포인트를 논리적으로 정의하는 부분.
우리가 쿼리를 날릴 때 이 카탈로그의 논리적인 부분을 명시를 해줘야 된다. 
그래서 카탈로그를 논리적으로 명시를 해주고 실행을 할 수 있다.

**스키마**

dbms에서 배운 데이터베이스라고 보면 된다. 테이블에 대한 그룹이라고 보면 된다. 

**테이블**

![6-2-3](https://user-images.githubusercontent.com/86764734/158184518-4a2e19a5-a826-4dc5-9d85-6b2e4886d685.png)

우리가 조회할 때는 논리적으로 카탈로그, 스키마, 테이블 이 세개의 형태를 이해하면 된다. 

#### 실습구성도

![6-2-4](https://user-images.githubusercontent.com/86764734/158184656-d9c75bd9-83d7-4e3b-9d3b-f85fdc7f392b.png)

이전에 우리가 전처리 단계를 통해 실버와 골드데이터를 생성해서 s3에 저장을 했다.

사실 골드데이터같은 경우는 dbms에도 되어 있는 상태. 그래서 s3에 있는 실버 데이터와 마트성 골드데이터인 dbms에 있는 데이터를 어떻게 조인을 해서 더 상세한 데이터로 만들 수 있을 지 고민해봐야 한다. 

이제 골드데이터를 보다가 의심가는 경우가 있는 경우 딥다이브를 어떻게 할 수 있는지 프레스토를 통해 실습!

전에 운영할 때는 프레스토를 통해서 타블로에 데이터를 올리는 태스크를 진행했는데,

athena의 경우는 똑같은 sql을 실행할 수 있다. (왜? 프레스토 엔진으로 이루어져있어서)

그리고 athena같은 경우는 서버리스이기 때문에 저녁에 필요한 시간에 배치를 돌리기에 가장 좋은 실행환경이라고 생각된다. 

그래서 athena를 통해서 타블로에 데이터를 적재해서 시각화하는 구조를 생각해서 athena하고 접속하는 방법에 대해 확인을 해볼 것.

athena 같은 경우는 현재 rds에 mysql에 데이터를 저장하고 있는데, 거기에 있는 데이터와 조인해서 분석할 수 있는 환경을 제공하고 있지 않기 때문에 개인적으로 분석할 때는 presto를 많이 사용

또 athena는 데이터의 용량으로 비용을 차징하고 presto는 컴퓨팅을 사용한 시간으로 비용차징을 하기 때문에 어떤게 이로울지 고민을 해봐야 한다!!