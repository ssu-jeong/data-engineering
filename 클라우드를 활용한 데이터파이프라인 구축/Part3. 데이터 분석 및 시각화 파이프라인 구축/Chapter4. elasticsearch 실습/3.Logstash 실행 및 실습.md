## Logstash 실행 및 실습

---

지난시간에 elasticserch를 하나 띄웠고 데이터를 elasticserch에 보내기 위해서 logstash를 ec2에 설치를 했다.

이번 시간에는 샘플데이터를 거기다가 생성을 해볼 것!

하루치의 로그데이터를 elasticserch에다가 쏘는 실습을 진행을 해볼 것이고, 그러기 위해서 s3에 제이슨 형태의 샘플 데이터를 생성하는 실습을 진행 할 것 

이 데이터가 2018년도 껀데 2021년도 꺼로 컨버팅하는 실습도 같이 진행!

### 실슴

---

#### EMR 접속

이전에 생성했던 클러스터 복제해서 생성.

#### Zeppelin 이동

zeppelin에서 작업을 진행할 것이기 때문에 **어플리케이션이력**에서 zeppelin 주소 복사하기

1. zeppelin을 브라우저에 띄워보자~

![4-3-1](https://user-images.githubusercontent.com/86764734/160286576-ecd5f0c9-53b0-469f-b363-ad8a44af5807.png)

1. 이전에 텍스트 파일을 이용해 josn으로 변환실습을 했던 그 파일을 불러와서 실습진행할 것. 
    
    import note 클릭해서 이전 실습파일 불러오기 
    
    → 2. readParseTXT-scala 이것은 텍스트 파일을 변환했던 소스코드
    
    이 소스코드를 읽어서 텍스트 혀태의 파일을 josn 형태의 파이롤 쓰는 그런 실습을 진행할 것이다.

    ![4-3-2](https://user-images.githubusercontent.com/86764734/160286587-c84f0486-50ce-4874-9992-32435f164db3.png)

2. 들어가면 s3의 위치를 변경해줘야 한다.
    
    하루치 데이터를 읽어서 실질적으로 똑같은 로그파일을 만들기 위해서 이 파일을 읽어서 josn파일로 쓸 것이다.
    
    어디다 쓸 것이냐면  fc-class/temp/json 폴더에다가!
    
    ![4-3-3](https://user-images.githubusercontent.com/86764734/160286595-d538ef02-0e1d-42e5-8c5e-dd7cc8012f26.png)

3. 파일을 읽자
    
    일단 파일을 읽었다. 그렇지 않으면 이제 logstash에서 프로그램을 더 복잡하게 해야한다.
    
    ![4-3-4](https://user-images.githubusercontent.com/86764734/160286601-2717334e-e321-44d2-8657-a57624dabe41.png)

    logstash를 배우는 것이 아니기 때문에 spark를 통해서 변환하는 작업 해보자!

    ![4-3-5](https://user-images.githubusercontent.com/86764734/160286612-79b880b4-48e4-415c-a9eb-b5a32c9b18a0.png)

4. sql에서 이제 해당 되는 컬럼을 읽는 방법
    
    logsDFAll.printSchema → 스키마 정보를 확인할 수 있다. 
    
    그래서 이것을 그대로 select문에다가 넣어주면 된다.
    
    ![4-3-6](https://user-images.githubusercontent.com/86764734/160286618-f933b9ed-cd07-4fc1-8733-b790e1c351e2.png)

5. spark SQL
    
    add_months(base_date,47) : 47개월을 더했다는 뜻 그럼 2021년 12월 26일이 나온다. 그런식으로 날짜 변환
    
    to_timestamp : logstash에서 elasticserch로 데이터를 전달을 할 때 이 형태가 timestamp 형태여야만 그것을 기준으로 해서 키바나에 대시보드에다가 시간별 추이를 그릴 수가 있다. 그래서 timestamp로 변환을 해줘야한다. 그리고 뒤에는 그 timestamp를 유지할 수 있도록 지정해준 것.

    ![4-3-7](https://user-images.githubusercontent.com/86764734/160286625-e67dc2e2-f976-4d71-b070-c55319d3a146.png)

6. 실행을 했는데 에러가 났다..!
    
    컬럼정보가 잘못되서 그런것 같음.. 아래 내용을 읽어서 수정!

    ![4-3-8](https://user-images.githubusercontent.com/86764734/160286630-caf9d8ad-68a0-4183-b02c-1a84f3f7ccb3.png)

    ![4-3-9](https://user-images.githubusercontent.com/86764734/160286646-af239704-ac01-410f-a8ab-e14b8d4e12e7.png)

7. 셀이 다 돌아가고 나면
    
    2021년 12월 26일 데이터가 된다. logstash를 통해 실습을 해보면 아는데 과거 데이터일 경우에는 거기에 스코프에 맞게끔 그쪽으로 기준날짜를 변경을 해줘야 한다. 불편하니까 최근날짜로 변환하면서 josn 형태의 데이터로 저장

    ![4-3-10](https://user-images.githubusercontent.com/86764734/160286654-c69602f9-4857-4fc5-9ed9-59b92e4e33fb.png)

#### JOSN으로 저장

1. spark sql을 그대로 프레임으로!
    
    앞의 sql을 spark에 넣어주면 얘가 데이터 프레임이 된다. 
    
    데이터 프레임을 josn으로 쓰는 명령어가 있다. 

    ![4-3-11](https://user-images.githubusercontent.com/86764734/160286661-255c633c-c811-45cf-b507-1a4f8290ee48.png)


    josn으로 변경.

    ![4-3-12](https://user-images.githubusercontent.com/86764734/160286670-d244fad6-5b08-450b-8b97-fe65f0851da9.png)

2. 타겟 위치는 copy   

![4-3-13](https://user-images.githubusercontent.com/86764734/160286678-aa927a5e-0377-4500-9f6f-071f98cbc0e2.png)

3. 실행 

    실행을 해주면 텍스트 파일을 json이라는 하위 폴더에 최근 날짜로 변환을 해주면서 josn 형태로 변환해서 새로운 데이터를 생성할 수 있다. 생성되면 이 데이터를 기반으로 elasticserch에다가 데이터를 보내서 실시간으로 어떻게 보는지 실습을 진행해보겠다. 

4. s3로 이동
    S3로 이동해보면,

    json/ 이라는 폴더가 생성된것을 볼 수 있고 들어가보면 json으로 변경된 파일이 확인된다.

    ![4-3-14](https://user-images.githubusercontent.com/86764734/160286691-1f165a05-e1b6-4547-958b-cd4a83d938a9.png)

    ![4-3-15](https://user-images.githubusercontent.com/86764734/160286698-72549044-bacc-4a95-b89a-817a057c51e6.png)

이 폴더를 잘 기억해줘야한다. 그래서 bucket temp밑에 josn이라는 폴더에다가 elasticserch로 실습할 데이터를 생성을 했다.  