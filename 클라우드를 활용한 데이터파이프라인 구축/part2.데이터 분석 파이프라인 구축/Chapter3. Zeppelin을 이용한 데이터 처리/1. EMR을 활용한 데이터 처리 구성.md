## 데이터파이프라인의 이해

![3-1-1](https://user-images.githubusercontent.com/86764734/153867027-4e682f07-6bd2-4566-b93f-9e593a4b8a92.png)

Bronze 데이터, S3에 쌓여있는 Bronze 데이터를 EMR안에 있는 spark를 통해 silver 데이터로 컨버팅하는 부분을 실습!

자세하게 설명을 해보면,

우리는 kinesis stream을 통해 데이터를 수집하고 Firehose를 통해 bucket에 저장했다. 

Rawdata(bronze데이터)가 버킷에 쌓였을 것이고, 이 버킷에 쌓인 데이터를 emr안에 있는 spark를 활용하여 transformed 해서 1차적으로 실버데이터로 transform data 버킷에 쌓는 그러한 분석에 대해 설명

 - 당연히 이 데이터는 모두 S3기반으로 이루어진다. 그래서 S3에 어느 수준의 데이터 까지 저장할지 고민을 해야할 부분 중 하나!

![3-1-2](https://user-images.githubusercontent.com/86764734/153867319-5f9cddad-21c1-4d2e-be1d-87b1013b7472.png)

### AWS EMR을 활용한 데이터 처리

![3-1-3](https://user-images.githubusercontent.com/86764734/153868118-5a0fde7c-ecd0-4656-9f47-c1106b2b7a48.png)

csv파일하나 가지고 파일을 읽고, 올프라미스에 있는 dbms파일도 export로 csv로 많이 내려준다. 
그럼 이 csv를 어떻게 데이터 프레임으로 컨버팅하는지, 데이터 프레임으로 변경하게 되면 

가운데 처럼 spark sql로 분석을 할 수 있수 있는 환경이 된다. csv파일이 여러개 있다면 각각을 하나의 테이블로 생각해서 조인해여 분석을 다양하게 할 수 있다!

csv파일을 읽어서 스파크 Sql로 분석하거나 또는 그 결과를 write로 쓰는데 dbms에다가 쓸수도 있고 JSON으로 저장할 수도 있고, 다시 csv파일을 통해서 내보낼 수도 있다. → 트랜스포메이션 

parquet 으로 데이터 저장을 할 건데, 이게 드라이브 없이 가장 흔하게 쓸 수 있는 데이터 타입으로 어떻게 보면 정형데이터에 가깝다.

csv같은 경우는 헤더정보정도 메타값으로 줄 수 있다, json같은 경우는 key-value 값으로 컬럼이름 정도 제공해줄 수 있는데 parquet 같은 경우는 컬럼정보 뿐아니라 데이터의 속성 정보도 줄 수 있다. 

또 압축을 제공하면서 빅테이터를 더 빠르게 연구하는 단계에서 발생한 압축파일형태이다. 
압축률은 집파일보다 떨어질 수 있지만, 스파크나 여러 분석 툴에서 파일을 읽을 떄 속도를 높여줄 수 있는 요소 
    → 그래서 parquet로 저장을 할 것이다!!! 하이브(하둡관련)은 잘 사용안한다. (txt data도 있을 수 있으니 해당 부분도. 로그파일이...?)

---

우리는 제플린을 통해 코딩을 진행할 예정.

![3-1-4](https://user-images.githubusercontent.com/86764734/153868917-171d0186-deba-4479-a5cd-c663e6af1b96.png)