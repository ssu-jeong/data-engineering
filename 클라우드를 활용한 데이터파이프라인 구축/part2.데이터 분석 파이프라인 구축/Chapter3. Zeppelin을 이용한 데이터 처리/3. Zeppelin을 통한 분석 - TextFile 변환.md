## Zeppelin을 통한 분석 - TextFile변환

---

텍스트형태의 파일 같은 경우는 더 많은 함수나 데이터 프레임으로 만들기 위해서 많은 작업이 필요하다.

앞서 배운 csv파일아나 제이슨 파케이 같은 것들은 기본적으로 골격이 데이터프레임의 형태로 되어 있는 파일 타입이기 때문에 쉽게 변환해서 핸들링이 가능한데 텍스트 파일의 경우는 어떻게 진행할까?

---
### Zeppelin

##### 1. 파일 열기
1. import note
2. select file - 3.2 readparseTXT-scala.json
3. S3에 가서 logs에 있는 파일 경로 확인하기

![Untitled](https://user-images.githubusercontent.com/86764734/154847195-9572bbe4-3b67-4c8d-a4f8-b4a276d9a323.png)

![Untitled (1)](https://user-images.githubusercontent.com/86764734/154847233-76adbbeb-d015-4a0e-baa9-e09253deb1b3.png)

##### 2. 노트열기

![Untitled (2)](https://user-images.githubusercontent.com/86764734/154847281-aafdbb7d-bdd2-4b99-95ea-78c3dfff8455.png)

![Untitled (3)](https://user-images.githubusercontent.com/86764734/154847296-a63fc4e6-bdda-43d8-a3b1-4e0eb325e65f.png)

- 앞에서 확인한 경로 복사해서 붙여넣기

![Untitled (4)](https://user-images.githubusercontent.com/86764734/154847312-d4b080da-dfed-45d3-8ae4-349bb856d3a7.png)

##### 3. 로그 안에 어떤 데이터가 있는지 확인

    - take(10): 그 로우 중에서 10개의 로우를 가져와서 보여줘. 

    - foreach(println): 각각을 뿌려달라는 의미.

    - 한줄씩 보게 되면 일단 파이프라인 형태로 구분이 되어 있다. 맨 끝에 마지막은 json형태로 하나의 로우가 구성되어 있다. 

    - json같은 경우에는 파라미터가 항상 요청에 의해서 추가가 쉽게 될 수 있다.

       -  테이블 같은 경우도 json형태로 들어오는 이유가 가변적인 그런 값에 대해서는 적용하기 힘들기 때문에 즉, static한 테이블에서는 적용하기 힘들어서 json형태로 저장을 하라고 되어있음

    - 그래서 json형태로 저장. 특정 파라미터에서 추가해 달라 삭제해 달라 하는 것은 json형태에서 처리를 해준다.  

    - 그 하나의 라인을 첫번째 데이터프레임 형태로 변환해야한다.

![Untitled (5)](https://user-images.githubusercontent.com/86764734/154847353-fff692b9-9ee0-4452-a63c-a163347c0448.png)

##### 4. line parser생성

![Untitled (6)](https://user-images.githubusercontent.com/86764734/154847386-cef782d5-8a80-4653-ae47-c785848c5dc1.png)

    - 저 하나의 라인을 읽어서 우리가 똑같이 데이터프레임 형태로 변경하는 작업을 진행한다.

    - 스칼라로 빠져있고, 그 스칼라에 해당하는 코드가 들어가있다.

    - 첫번째로 클래스 정의(보통은 클래스로 정의하지 않아도 된다. )

    - 저 들어가 있는 항목이 22개가 넘어가면 에러가 발생 → 이럴경우는 클래스를 미리 정의한 다음에 클래스에 데이터를 하나씩 넣어줘야 한다. 

    - val pleces = line.split(”\\|”)
            
            → 하나의 로우에서 ( | )이 파이프라인 형태의 특수문자로 스플릿하는 작업을 하고, 맨 마지막은 json형태(//JSON parse 부터)
            
            첫번째 위치의 데이터가 adid. 
            두번째는 uuid. 
            
            약속된 데이터들을 넣어준다. 
            차례대로 나뉜 부분을 넣어주는 작업을 해준다. 하나의 func 에서 
            
            마지막은 JSON형태라서 그 형태로 변형시켜주는 작업을 진행한다. → (//JSON Parse)부터 
            
            JSON 형태이기 때문에 데이터를 하나의 컬럼형태로 만들어주는 작업이라고 보면 된다.

<img width="1027" alt="스크린샷 2022-02-14 오전 11 44 40" src="https://user-images.githubusercontent.com/86764734/154847421-44665976-f0a1-4005-8671-1c6084f61f67.png">

##### 5. 그 메소드를 하나씩 실행하는 작업

    - map 메소드라고 하는데 위에서 정의된 parsRaqJson이라는 하나의 라인을 함수를 호출해서 변환하고 그것을 데이터 프레임형태로 변환(toDF())

    - 그다음에 sql로 데이터를 핸들링하는 작업을 해야하기 때문에  데이터 형태로 변환하는 작업을 실행한다. 

![Untitled (7)](https://user-images.githubusercontent.com/86764734/154847449-ba7e14d7-6ea0-4df8-bed3-36f93e75d61f.png)

##### 6. ‘line parser 실행’ 

- 실행을 해보기 전에, ‘line parser 실행’ 작업줄을 실행하고 galglia를 띄어놓고 작업을 하게 되면 중간 중간 리소스를 얼마나 사용 하는지를 확인하며 작업할 수 있다. 

- sql 작업까지 실행하면 전체파일을 로딩을 하기 때문에 galglia에서 모니터링이 되지만, 바로바로 올라오지 않아서 시간을 두고 확인해야한다.

![Untitled (8)](https://user-images.githubusercontent.com/86764734/154847489-1b056da8-cdc9-4860-a1cf-d639cb62a5bf.png)

##### 7. 모니터링 - galglia
    1. 색이 변한 것을 확인 할 수 있다. 계속 무거워지는 것을 확인할 수 있다.

<img width="647" alt="스크린샷 2022-02-20 오후 11 26 59" src="https://user-images.githubusercontent.com/86764734/154847538-b8f8f24b-2aca-4cfd-8682-5142b61c4086.png">

![Untitled (10)](https://user-images.githubusercontent.com/86764734/154847566-f8a269a8-d0b1-42f6-b904-abf74b5a6858.png)

- 노드가 현재 4개 마스터 노드1개와 워커 노드(코어, 태스크)에 해당하는 노드가 3개로 구성된 것을 알 수 있다. 

- 파일이 현재 크지 않기 때문에 빨간색으로는 확인되지 않는다 

이런 식으로 큰 데이터를 핸들링 할때는 galglia를 보면서 모니터링 한다.

-> 빨간색으로 변하면 노드가 모자라구나를 확인하고 스케일 아웃진행하면 된다. 

### 정리

---

오늘은 우리가 어떻게 텍스트 파일을 컨버팅해서 작업할 수 잇는지 알아보았다. 

모니터링을 위해 count진행 한 것이고 sql을 통해서 조회를 할 수 있다. 

*탭으로 구성을 하면 로그에 대한 내용과 비지니스 내용 그리고 앱에 대한 네비게이션이나 여러가지 형태를 이해하고 분석을 진행하게 되고, 여러가지 인사이트를 찾기 위한 환경이 구성된다. 

![Untitled (11)](https://user-images.githubusercontent.com/86764734/154847584-551c57fa-48ca-4521-adc0-ee744dd4d95d.png)

다음은 접속해서 디비에 데이터를 쌓는 법을 공부하자!