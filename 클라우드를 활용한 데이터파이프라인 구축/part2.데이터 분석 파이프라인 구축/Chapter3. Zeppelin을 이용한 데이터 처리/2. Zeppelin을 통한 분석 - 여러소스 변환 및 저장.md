## Zeppelin 켜기

EMR에서 어플리케이션이력에 가서 url복사 새로운 탭에 붙여 넣으면 노트북 확인 

![3-2-1](https://user-images.githubusercontent.com/86764734/154089927-072ad977-eb6f-41d3-bb59-c9eec7d8e50e.png)

서브메뉴인 notebook을 클릭하면 밑에 있는 노트북을 실행할 수 있는 페이지!

![3-2-2](https://user-images.githubusercontent.com/86764734/154090070-cd12aa1b-881c-4d02-bf3b-af1aa4bb474b.png)

job을 누르면 우리가 실행하는 잡들을 모니터링 할 수 있는 기능

![3-2-3](https://user-images.githubusercontent.com/86764734/154090198-a309e6cc-b737-42d7-b916-729d308976c4.png)

차후에 intrpreter 기능을 추가할 수 있는 기능도 있다.

![3-2-4](https://user-images.githubusercontent.com/86764734/154091365-cfdaa240-f961-4f19-a37d-6556b7abe665.png)

클릭해보면

![3-2-5](https://user-images.githubusercontent.com/86764734/154091464-08c5e705-96b0-4bdf-bfb6-5db851bc71bf.png)

### Intepreters

현재 zeppelin에서 지원해주고 있는 것들

![3-2-6](https://user-images.githubusercontent.com/86764734/154092123-6e2902fa-fe5d-4cf3-95f4-992483acf173.png)

- juputer

![3-2-7](https://user-images.githubusercontent.com/86764734/154092151-5dda2bc9-4670-4da2-825b-75ab64aa012c.png)

- kotlin

![3-2-8](https://user-images.githubusercontent.com/86764734/154092173-be5a9ed5-9332-4b48-a26c-16b4b650e5ed.png)

- livy

![3-2-9](https://user-images.githubusercontent.com/86764734/154092193-4f3b4ac1-a8a3-4342-9fa0-2f0a52c93337.png)

- md -> 커맨드창

![3-2-10](https://user-images.githubusercontent.com/86764734/154092208-038892e0-5ea6-4ee9-bb82-06100975b181.png)

- python

![3-2-11](https://user-images.githubusercontent.com/86764734/154092224-65ebc34c-a475-452b-8d30-998eca7c6c7e.png)

- r

![3-2-12](https://user-images.githubusercontent.com/86764734/154092234-e2339d1c-c527-454a-bc74-62e76821c931.png)

- sh

![3-2-13](https://user-images.githubusercontent.com/86764734/154092254-7fd7f45b-e645-4950-a0c6-a0cccf0b0fe3.png)

- spark

![3-2-14](https://user-images.githubusercontent.com/86764734/154092265-14726be3-7ef7-44f4-a54b-b9c3bb50ca63.png)

이 외에도 다양한 것들을 사용할 수 있다.

---


### 새로운 노트북 생성

- create new notebook

![3-2-15](https://user-images.githubusercontent.com/86764734/154093268-3494d40d-1441-4cff-9117-2ba41039b4a9.png)

우리는 강의용으로 주어진  데이터가 있기 때문에 import note 클릭

- 두가지 메뉴 중에서 select json 클릭해서 제공받은 파일 선택

![3-2-16](https://user-images.githubusercontent.com/86764734/154093566-8185957e-ca0a-4d80-8090-ba831392a046.png)

이전에 만들어놨던 소스들인데, 우리는 수정된 소스를 보고 우리들의 환경에 맞게 컨버팅해서 사용하면 된다.

소스코드는 scala로 되어 있다 scala 신텍스 중에 3분의 2정도만 이해하면 스파크에서 충분히 사용가능

scala라고 써있는 소스를 오픈!!

![3-2-17](https://user-images.githubusercontent.com/86764734/154093669-ea742d8f-50f9-4cd9-a244-7f92459421c1.png)

클릭해서 열면 해당 화면이 확인된다.

![3-2-18](https://user-images.githubusercontent.com/86764734/154093779-a9ceaee0-f6e7-460c-b634-dad96e99838a.png)

이전에 s3으로 되어 있는 부분을 이렇게 확인가능   

이전과 다르게 스파크가 캡슐화라고 할까, 즉흥으로 csv파일을 읽는다.

![3-2-19](https://user-images.githubusercontent.com/86764734/154093923-96cc2625-068e-47d5-ab97-dd0017c6c028.png)

우리 로컬에 있는, 나의 S3에 있는 단지마스터.
csv을 읽어와야 하기 때문에 이것을 샘플로 해서 컨버팅하는 작업을 실습

![3-2-20](https://user-images.githubusercontent.com/86764734/154094051-b964bf4b-b3dd-4b05-9932-ddc9f15cd3e1.png)

ods라는 곳에 넣어놨으니 해당 폴더로 이동.

![3-2-21](https://user-images.githubusercontent.com/86764734/154094231-84819869-1fd3-4bfa-9211-7eaab467065e.png)

db에 있는 것을 그대로 내렸기 때문에 ods라는 폴더를 만들어서 넣었다.

여기서 단지마스터를 클릭해서 들어오면 과거에는 없었지만 url복사가 가능하다

![3-2-22](https://user-images.githubusercontent.com/86764734/154094488-a304e27e-427a-441d-abd1-07a96bf6e889.png)

복사한 주소를 붙여넣는다

![3-2-23](https://user-images.githubusercontent.com/86764734/154094610-49674a08-298d-4527-ab90-bdc846bd1d1d.png)

좀 더 설명을 하자면 csv파일은 첫번째 라인의 첫번쨰 로우에 헤더값이 있는 경우가 있고 없는 경우가 있는데, 헤더가 있다면 트루로 설정하여 첫번째 로우를 컬럼에 컬럼명으로 넣어준다.

두번째 줄은 어떤 구분자로 데이터가 구분되어 있는데 delimiter를 컴마(,)로 구분하여 데이터 프레임 형태로 읽어온다. 

세번째는 inferschema → true는 전체데이터를 읽고 숫자만 있으면 인티저로, 스트링은 스트링으로!

### S3에 있는 csv파일 읽기

// 이렇게 하면 주석처리, 시프트 엔터하면 실행

![3-2-24](https://user-images.githubusercontent.com/86764734/154291916-816d83bd-e75b-46ca-8b6f-4355fc2c6b14.png)

우리가 초기에 실행을 한게 컴파일이다. 
그 신텍스 에러나는 부분을 체크해주는 것.

최초실행시 속도가 느리다 왜?

→ 제플린이 인터프리터라서 스파크와 세션 통신을 열어야하기 때문에

그 다음 실행해 주면 어떤 데이터가 있는지 확인 가능 

헤더를 트루로 놨기 때문에 첫번째 줄에 컬럼정보가 있어서 헤더값으로 들어왔다.
![3-2-25](https://user-images.githubusercontent.com/86764734/154292038-018587d9-789d-46fe-9696-a539b4309372.png)

그 다음 데이터 타입확인한다.

전부 다 string으로 되어 있는 것을 확인 할 수 있다. 왜냐면 파일 정보가 어떤건지 모르기 때문에 스트링으로

![3-2-26](https://user-images.githubusercontent.com/86764734/154292215-154096c3-b5da-42d4-8849-dd4705068c61.png)

### 인퍼스키마옵션

주석 옵션을 풀고 다시 실행 (인퍼스키마를 트루로 놓고!)

아이디가 인티저로 변경된 걸 확인 할 수 있다. 아마 id는 모두 숫자로 구성되었다는것!

즉, 각 컬럼의 데이터 타입을 거기에 맞게 변환시키길 원한다면 옵션을 주고 실행하면 된다. 
단, 파일이 큰경우 파일 전체를 다 읽고 로딩하여 타입을 지정하기 때문에 오래 걸릴 수 있으니 생각하고 작업을 진행해야한다.

![3-2-27](https://user-images.githubusercontent.com/86764734/154292512-65e36be1-1bfb-42c5-9592-c9cfd43c74ef.png)

스파크는 데이터 프레임 형태 데이터 프레임 형태는 엑셀로 치면 하나의 워크시트, 

앞에서 데이터가 어떤 형태인지, 컬럼이 있는지를 확인 했다. 이걸 우리가 쉽게 테블 형태로 바꿀 수 있다.

왜냐면 데이터 핸들링은 우리가 sql을 통해서 했을 때, 더 쉽게 진행 할 수 있고 거기에 필요한 데이터를 뽑아서 다른 형태로 트랜스포메이션 할 수 있기 때문에 보통 테이블 형태로 변경

그리고 우리고 보통 핸들링할 때 여러개의 테이블을 조인해야 하기 때문에 보이는 것 처럼 데이터 프레임을 테이블로 변경해주면 데이터에 대한 변환을 쉽게 할 수 있다.

#### 테이블로 변경

- 뒤에 내가 쓰고자하는 테이블명을 정해준다.

![3-2-28](https://user-images.githubusercontent.com/86764734/154292653-524066e0-2e4b-4caf-ae25-2afed9d602e4.png)

#### 뷰테이블 확인

- %sql이라고 하면 바인드 중에서 spark sql을 쓴다는 의미 

그러면 테이블 명이 확인된다.

![3-2-29](https://user-images.githubusercontent.com/86764734/154292832-77f83483-c2a0-4eb4-919b-d8d386c9cb89.png)

이즈템포러리 → 이 테이블은 생성해서 우리가 핸들링하고 놔두면, 이 세션이 끊기면 자동적으로 사라지는 테이블!

**스파크sql 수행**

![3-2-30](https://user-images.githubusercontent.com/86764734/154293013-253267d0-83a5-4e8b-9ccf-be24d0aef9e4.png)

**groupby실행**

![3-2-31](https://user-images.githubusercontent.com/86764734/154293151-d52b84f4-1ae9-4d85-ba77-a2e2c3c717ff.png)

지금 단계에서는 여러 데이터 포멧을 컨버팅하는 작업을 진행해보는 것이라 여기까지만 확인!

#### CSV 데이터를 JSON으로 만들기

스파크 자체가 여러 노드를 가지고 distributied 컨버팅을 하기 때문에 만약에 여기에 cloalesce라는 옵션을 주지 않으면 json이라는 파일이 여러개 생성이 될것!

먼저 해당 func 주석처리하고!

- mode → overwrite와 append가 있다 overwrite는 기존에 똑같은 파일이 있으면 기존파일은 삭제하고 새로운 파일을 쓰게된다.
- 포멧은 제이슨 형태
- 해당 위치에 저장을 한다. ods의 테스트

우리가 지금은 파일형태를 그대로 컨버팅하는거지만, sql을 통해서 여러가지 조건을 주고 그 결과를 저장하고 싶으면 그대로 데이터프레임으로 만들어서 컨버팅할 수도 있다.

![3-2-32](https://user-images.githubusercontent.com/86764734/154293772-b23ac96a-7fc0-44b4-8c6a-24c0189de6fd.png)

그리고 나서 S3에 ods폴더에 가보면 test라는 폴더가 생긴 것 확인 가능.

![3-2-33](https://user-images.githubusercontent.com/86764734/154293862-78c15be2-af9a-4820-94ad-b285f7a86c55.png)

테스트 폴더에 가보면 제이슨 형태로 저장된 폴더를 볼 수 있다.

![3-2-34](https://user-images.githubusercontent.com/86764734/154293966-1d08d0c4-6ce0-4179-b479-f91cc9fa4bb8.png)

현재 파일이 크지 않아서 하나의 프로세스에서 작업된것 을 확인 할 수 있다. 

![3-2-35](https://user-images.githubusercontent.com/86764734/154294194-28e6d585-fcc0-4315-90b8-e4a7858be66b.png)

### JSON 형태 읽기

앞에서 저장하였던 JSON 파일의 위치를 복사해서 밑에 그대로 붙여넣는다. 그러면 JSON 형태의 데이터프레임을 확인 할 수 있다. 

특별히 트랜스포메이션 하지 않았기 때문에 똑같은 모양으로 확인된다.

![3-2-36](https://user-images.githubusercontent.com/86764734/154804817-255bafde-5ca6-404f-8581-7348899a586e.png)

그리고나서 스키마 값 확인

아이디만 long 값으로 잡힌다. 나머지는 스트링 즉, 제이슨 형태도 모든 데이터를 확인하여 스키마 값을 결정하는 것을 알 수 있다.

![3-2-37](https://user-images.githubusercontent.com/86764734/154804853-245fb918-20c2-4bbc-86ee-ae44c05d22fa.png)

이것도 우리가 똑같이 테이블 형태로 만들어 줄 수 있다.  테이블로 만들고 조회를 해보면 똑같은 결과를 확인 할 수 있다.

![3-2-38](https://user-images.githubusercontent.com/86764734/154805112-607f1231-ba0c-4ea1-979a-f8ac7e1822ac.png)

우리가 앞에서 테이블을 지정해준 것이 있기 떄문에 현재 두개가 확인되어야한다

![3-2-39](https://user-images.githubusercontent.com/86764734/154805155-33688916-2291-4305-a9e8-289f4221f9a7.png)

이렇게 서로 다른 파일들을 읽어서 우리가 뷰테이블로 만든 다음에 조인을 통해 분석이 가능하다. 

### parquet 데이터 타입

위치는 위에와 똑같이 설정해 줄 것이기 때문에 복사해서 붙여 넣어준다. 

보통 파일이 클 경우에 스파크 같은 경우에도 동시성이 떨어질 수 있다.

```java
//Write Parquet
jsonDF.write
      .mode("overwrite")
      .format("parquet")
      .save("s3://폴더/이름/넣어/주기/")
```

해당 셀 실행하고 s3로 이동하면 파일이 확인된다.

원래는 압축을 해줘야 하는데 aws에서 parquet로 저장하게 되면 snappy형태로 압축이 되서 저장된다.  snappy도 zip처럼 똑같은 형태 

**snappy**: 빅데이터가 생기면서 효과적으로 압축률을 줄이면서 읽는 것을 빠르게 하기 위해 나온 압축형태

![3-2-40](https://user-images.githubusercontent.com/86764734/154805331-7e805598-ab8e-4a6e-ad8c-55dc69ecd721.png)

### Parquet 데이터 읽기

데이터를 읽어서 스키마 정보 확인 

id값만 long값으로 나오고 나머지는 스트링

```java
//Read Parquet
val parquetDf = spark.read.parquet("s3://폴더/이름/넣어/주기")
```

parquet형태의 데이터도 똑같이 데이터프레임 형식으로 바꿀 수 있다. 

```java
//parquet -> DF
parquetDF.createOrReplaceTempView("데이터이름")

//데이터확인
%sql
select *
from 데이터이름
```

다양한 형태의 데이터를 핸들링하는 법을 배울 수 있었으면 된다 

그럼 parquet 값을 가지고 다시 csv파일로 써보도록!

먼저 collesce 주석 처리하고!

![3-2-41](https://user-images.githubusercontent.com/86764734/154805538-6b7d36b6-71a5-4e49-9f95-f9c35971ee85.png)

s3로 가서 보면 csv하나로 생성된 것을 확인 할 수 있다. spark에 정의된 블록보다 사이즈보다 파일이 작기 때문에 하나의 파일로 생성.
결과가 큰 파일로 구성이 되어 있으면 여러개의 파일로 결과가 나올 것이다. 그래서 collesce 옵션이다.
디파티션 옵션을 주고 생성해야한다. 

![3-2-42](https://user-images.githubusercontent.com/86764734/154805571-69870945-367f-4eaa-b93d-a265e23d2913.png)

주석을 풀고 다시 실행하면 오버라이팅옵션이 있기 때문에 그 자리에 그대로 새로운 데이터가 저장

![3-2-43](https://user-images.githubusercontent.com/86764734/154805605-af6b4dff-7a2e-4072-acb5-d1a3fca14b49.png)


**정리**
보통 csv파일로 떨궈주는 경우가 많다. 
변환 작업을 해서 dbms에 저장할 수도 있고 s3에 저장해서 쓸 수도 있다.