## 실버데이터 생성

---

aws콜솔로 이동해서 실습 진행하도록 한다.

1. 서비스 서치엔진에서 emr들어가서 클러스터 하나 띄우기
    
    - 될 수 있으면 우리가 실습할 때 스텝에서 스크립을 실행한 것들이 있기 때문에 복제해서 진행한다.

    - 복제 완료되면 이전 클릭해서 처음 단계인 소프트웨어 구성으로 이동하자!

![5-2-1](https://user-images.githubusercontent.com/86764734/156108642-59775270-c6b8-42f1-86a5-e1254db9a181.png)

2. 모든단계(선택사항)
    
    우리가 스텝단계에서 실행했던 스크립들이 등록되어 있다. 
        
    - 첫번째는 galglia
        
    - 두번째는 mysql드라이버 설치하는

![5-2-2](https://user-images.githubusercontent.com/86764734/156108899-762505ab-2ae8-49f5-adb4-759c83625de8.png)

- 에코시스템에 소프트웨어 설정이 되어 있다.

![5-2-3](https://user-images.githubusercontent.com/86764734/156108948-f300f5b7-9467-4e39-857d-fbb913036225.png)

- 중요한건 메타스토어 역할을 하는 데이터카탈로그를 활용을 할 것이기 때문에 해당부분 꼭 체크!

- 근데 외부 db에 메타스토어를 운영하는 경우도 있는데 관리포인트를 줄이기 위해 glue에 있는 카탈로그를 이용하는 것을 추천

![5-2-4](https://user-images.githubusercontent.com/86764734/156109001-852f3dc7-dab3-440f-a666-f55c5eed4502.png)

- 또 한가지 중요한 것은, 아테나 자체가 프레스토로 라는 에코시스템으로 구성되어 있는데

만약 프레스토를 사용하겠다고 체크를 하면 프레스토 자체도 메타스토어로 쓰겠다고 체크를 해줘야 프레스토에서 sql를 날릴 때 메타스토어를 참고해서 sql을 통해 분석을 진행할 수 있다.

![5-2-5](https://user-images.githubusercontent.com/86764734/156109102-06d0a78b-92ba-4965-910d-91ac3c501a77.png)

3. 노드와 인스턴스
    
    꼭 온디맨드 → 스팟 으로 변경하기!!!!!

![5-2-6](https://user-images.githubusercontent.com/86764734/156109209-d8fc93fe-f15a-4e2c-bbc5-63339bcf77ca.png)

- 확인 할 것은 초록색이 무엇인지 확인하고 서브넷 변경해주고 스팟인스턴스를 실행해줘여 된다. 

![5-2-7](https://user-images.githubusercontent.com/86764734/156109267-cd4aebd1-e681-450d-8aa0-43b02904ade9.png)

4. 클러스터 생성
    
    클러스터가 다 구성되고 나면 어플리케이션 이력에 가서 제플린 URL복사해서 들어가면 된다.

![5-2-8](https://user-images.githubusercontent.com/86764734/156109378-440a006d-3ad7-4b5c-8bfa-23bb6d84083f.png)

- 만약 위치가 변경된다면??

마스터보안에서 설정해주자. 대부분 에코시스템은 마스터에 있다.

![5-2-9](https://user-images.githubusercontent.com/86764734/156109799-ddb93c2f-f33e-4f9c-b520-b72950e31e0f.png)

- 인바운드 큐칙을 확인 후 없으면 인바운드 규칙편집

    - 모든 TCP/ 사용자IP/ 설명-my office

5. 어플리케이션 이력에서 zeppelin 카피해서 실행.

![5-2-10](https://user-images.githubusercontent.com/86764734/156110005-44f5e9d6-70d7-41c8-a6eb-1d8868c5a492.png)

6. 제프린 열기
    
    지금 미리 파일 불러와 있어서 확인되는 것. 우리는 import note에서 파일을 불러와야 한다.

![5-2-11](https://user-images.githubusercontent.com/86764734/156110163-4aead17c-85e7-48dd-be79-a897177dffdc.png)

### 단지별 사용자 조회수

1. 데이터 수집 관련된 아키텍쳐를 했을 때, 수집된 이벤트 로그가 S3에 있는 logs라는 폴더에 쌓일 것이라고 가정한다. 이것은 하루치의 데이터 이며 용량은 1.5기가 정도 된다. 

![5-2-12](https://user-images.githubusercontent.com/86764734/156111030-1923defb-a097-429f-9dbe-e5cb0a719d74.png)

2. 데이터 용량확인
    압축이 되어 있는 상태에서 용량이 1.5GB인 것을 확인할 수 있다. 약 10기가정도 되지 않을까 싶다. 이 데이터는 **브론즈 데이터**!

![5-2-13](https://user-images.githubusercontent.com/86764734/156111185-0e708569-782f-4300-8cf2-88ca8e4af900.png)

![5-2-14](https://user-images.githubusercontent.com/86764734/156111478-d6230534-b3d8-494a-b8cf-4474f575333b.png)

3. 지난번과 다르게 추가된 부분이 있을 것.
    
    이거는 우리가 hive테이블을 선언을 한 후에 여기에 데이터를 인서트해서 데이터를 넣을 것이라서 거기에 필요한 옵션 두가지.
    
    dynamic.partition -> 여러 폴더가 있으면 각각 나눠서 데이터를 분리해서 넣을 수 있는 옵션!

![5-2-15](https://user-images.githubusercontent.com/86764734/156111595-0e4d0db9-b203-4e87-b9f8-cb8f79b44d44.png)

4. 각각 라인을 파싱하는 소스코드
    
    이걸 통해 로그파일을 확인을 하고 기능을 짜야한다. 근데 이제는 데이터를 쌓을때 대부분 파일의 크기가 좀 커도 json 형태로 저장을 하는게 가장 유연하게 로그관리를 할 수 있다. 
    
    그렇지 않rh static하게 되면 배포할 때 변경작업이나 수정작업을 반복적으로 해야하기 때문에 그런것 없이 저장하는건 json 형태가 가장 좋다.
    

![5-2-16](https://user-images.githubusercontent.com/86764734/156111803-bbe7a799-0f5c-44aa-8ec9-5fc31c0d74cd.png)

5. **sqlcontext.cachetable**
    
    → glanglia를 띄어 보면 캐시영역이 잇는데 이 캐시영역에 데이터를 올릴 때 캐시로 올리는 옵션
    
    이 캐시 주는 부분은 데이터프레임형태를 주는 경우가 있고 이거는 logs라는 테이블을 tableview를 하나 만들어서 캐시를 주는 형태 근데 캐시된 테이블과 안된 테이블 두개가 있을 경우에는 대부분 캐시가 안된 것 기준으로 되기 때문에 두개의 테이블이 있을 땐, 두개나 캐싱을한 후에 sql을 날려야 한다.

![5-2-17](https://user-images.githubusercontent.com/86764734/156111986-0d8de45b-7a6e-41fb-83e0-5e8644cb39f1.png)

```
val spark: SparkSession = 
            SparkSession
            .bullder()
            .appName("StatsAnalyzer")
            .enableHiveSupport()
            .config("hive.exec.dynamic.partiton", "true")
            .config("hive.exec.dynamic.partiton.mode", "nonstrict")
            .gettOrCreate()
```

![5-2-18](https://user-images.githubusercontent.com/86764734/156112102-14e68356-6b70-4910-95b7-d21ff1a75f74.png)

6. 데이터프레임에 대한 데이터확인 

```java
logsDRAll.show
```

이렇게 캐싱을 하면 s3데이터를 다 캐시로 로딩을 해야한다. 그래서 전체에 대한 카운팅을 해줘야 모든 데이터가 올라간다.  

그래야 그 다음부터 좀 빠르게 분석을 할 수가 있다. 해당 셀을 실행을 하고 ganglia를 통해 모니터링을 계속 해주자! 

캐시자체자 조금씩 올라가는 것을 볼 수 있다. 카운팅을 한번 해주면 테이블에 캐싱이 될 것이고 얘가 마스터성 테이블이면 배치돌 때 얘를 한번 카운트 all을 돌리고 그것을 기준으로 해서 나머지 밑에 쿼리들이 캐시에 대해서 적용받도록 플로우를 짰다. 

- 카운팅

```
%sql

select count(*)
from logs
```

8. 데이터 보기
    
    우리가 아파트에 대한 단지조회에 대해 저장을 진행할 것. 다른 테이블과 조인해서 이 데이터를 풀게 아니고 우리가 실버데이터를 만들것이기 때문에  카탈로그가 아파트인 것만 불러온다. 

```
%sql

select *
from logs
where item_category = '아파트'
```

9. 아파트상세를 여러 각도로 분석
    
    다양한 조건을 쿼리문을 통해 확인해보자.
    
    옆으로 넘겨서 다양한 컬럼을 확인하자
    
    이렇게 logs를 테이블뷰를 통해 확인

```
%sql

select *
from logs
```

11. 사용자에 대한 아파트단지 조회
    
    그다음에 우리가 정의할게 사용자에 대한 아파트 단지 조회에 대한 것이니까 그런 조건을 넣어주었다.

   zeppelin에서 쉽게 할 수 있는게 시각화를 들 수 있다. 아래처럼 바차트로 설정해서 보면 좀 더 시각적으로 이벤트의 흐름을 알 수가 있다. 

![5-2-19](https://user-images.githubusercontent.com/86764734/156116796-2bce99fb-41bd-4173-b439-a7b888f1b1ed.png)

12. 우리가 목적하는 바는 아파트단지조회!
    
    우리가 쌓는 방법은 기준일자별로 사용자가 이 단지를 언제 조회 했는지 로그데이타 기반으로 정리해서 저장을 할 것이다. 4개의 컬럼만 뽑아서 저장!

### parquet 형태 저장
    
parquet로 저장하는 소스
    
초기에는 Hive때는 메타스토어 자체가 없기 때문에 parquet로 저장해서 읽는 방법을 많이 썼었다. 
    
코드를 해석해보자면,
    
sql을 만들어서 sqlcontext해서 sql을 넣고 실행을 하게 되면 하나의 데이터프레임이 생성이 되고, 해당 프레임을 S3에 parquet 형태로 저장을 하는 소스코드.

![5-2-20](https://user-images.githubusercontent.com/86764734/156369175-39a82d0c-0b7a-4798-b79a-fb13dd0d4654.png)

저장방법은 S3로 가서 폴더를 만들어주자. 

![5-2-21](https://user-images.githubusercontent.com/86764734/156369328-09c8585f-0d88-45a3-9e75-b27a36140e48.png)

만들어 준 실버 폴더에 가서 url복사!

![5-2-22](https://user-images.githubusercontent.com/86764734/156369493-1c873787-bc6d-44a8-8115-e459622fbf93.png)

복사한 주소를 붙여 넣어준다.

parquet를 저장할 때는 테이블형태처럼 똑같이 폴더를 정의하면 된다. 그러면 danji_user_view라는 폴더가 생길거고 그 안에 여러개의 파일이 생성될 것이다.

그 이유는 spark 자체가 여러 노드들이 돌면서 파일이 생성되기 때문에 그 노드별 생성된 파일이라고 보면 된다. 

그래서 특정한 옵션이 없으면 프로세스 단위로 파일이 하나하나씩 생길것!

그래서 우리가 분석한 것을 저장하는 부분

![5-2-23](https://user-images.githubusercontent.com/86764734/156369597-328e7326-8a8d-4be6-a56c-753576538251.png)

2. 저장된 것 확인!
    
    다시 s3로 돌아가 새로고침 해보면 아까 저장하려는 폴더가 생긴 것을 볼 수 있다. 이게 하나의 테이블 처럼 생각을 하면 된다.

![5-2-24](https://user-images.githubusercontent.com/86764734/156369741-72093847-60a9-4b63-a46c-a6126873c9ba.png)

클릭을 해보면 snappy는 압축방식이고 parquet 형태의 데이터들이 쌓인 것을 볼 수 있다. 

![5-2-25](https://user-images.githubusercontent.com/86764734/156370116-33fff6a8-b3e0-4406-bef7-3bc582891e39.png)

소스코드를 통해서도 데이터가 잘 쌓였는지 확인 가능 

![5-2-26](https://user-images.githubusercontent.com/86764734/156370253-3cbafb0d-fa0f-418a-b219-1e80abdd5633.png)

근데 이렇게 데이터를 쌓는 방식은 약간 좀 느리다. 

### hive sql로 저장

hive에서 테이블을 생성하고 인서트하는 방법을 진행해보자.

hive같은 경우는 테이블을 미리 크레이션 해아하는데, 방법은 3가지 정도라고 생각하면 된다.

우리가 지금 spark에 데이터 카탈로그를 연결을 해놨다.
그래서 테이블을 여기서 크레이션 해도 생성이 된다.

![5-2-27](https://user-images.githubusercontent.com/86764734/156370445-8adfe939-4710-42a8-b5a8-7e715de590f4.png)

또 다른 방법은 glue에서 진행할 수 있다.
저희가 크롤링하면 테이블이 생성이 되었죠? 
그래서 앞으로는 클래스에다가 테이블을 만들건데, **테이블수동으로 추가** 하기를 통해 만들어 줄 수 있다. 컬럼을 넣고 하나하나 추가해주면 되는데 이건 엄청 오래걸린다. 


![5-2-28](https://user-images.githubusercontent.com/86764734/156370523-e3088341-2632-47e4-b470-2f62c01b1912.png)

또 다른 방법은 athena를 통해 크레이션 하는 방법이 있다. 

1. 우리는 zeppelin을 통해서 spark 자체에 테이블을 생성해보도록 하자! 
    
    → 나눠준 4주차 실습 파일에서 스크립을 확인해서 테이블을 크레이션 하고 athena에서 확인해보자!
    
    EXTERNAL : 이 hive테이블을 생성을 할 때, 외부에 저장을 하겠다는 것. 스토리지를 하둡에다 저장하는 것이 아니라! 그래서 로케이션 자체를 s3에 저장하도록 했다. 
    
    class.danji_user_view_silver : class라는 테이블을 만들었고 danji_user_view_silver가 테이블 이름
    
    _silver : 데이터 형태에 대해서 테이블이름에 명시하는 경우도 있다. 
    
    PARTITIONED : 파티셔닝은 날짜별로 폴더가 생성이 되고 그 밑에 데이터들이 쌓이게 되는 옵션. 이것은 이제 쌓일 떄도 각각 날짜별로 쌓이게 될 것이고 조회할 때도 날짜에 조건을 주면 해당 날짜의 데이터만 읽어오기 때문에 (dbms같은 경우도 날짜별로 파티셔닝옵션을 줄 수 있는데) 좀 더 속도 있게 조회하기 위한 방법이라고 보면 된다! 꼭 파티셔닝을 줘라! 근데 파티셔닝을 너무 많이 주게 되면 저장할때 느려질 수 있으니 주의해서 옵션 설정해라. 파티션을 너무 세분화하니까 느려지더라
    
    STORED :  parquet 형태로 저장 
    
    tblproperties : 압축방식은 snappy, 분류는 parquet로 구분하여 저장하도록 했다.

```java
%sql
CREATE EXTERNAL TABLE class.danji_user_view_silver(
   base_dt timestamp, 
   adid string,
   danji_id int)

PARTITIONED BY(base_date data)
STORED AS PARQUET
LOCATION ''
tblproperties("")
```

![5-2-29](https://user-images.githubusercontent.com/86764734/156370827-4d2e19c9-f0b6-44ea-a9dc-a91da38b4748.png)

실행을 하면 테이블이 생성이되었을 것. glue에 이동해서 데이터카탈로그를 확인해보자.

![5-2-30](https://user-images.githubusercontent.com/86764734/156370930-c553f5e6-b0b2-4416-b63d-4c00640873a9.png)

![5-2-31](https://user-images.githubusercontent.com/86764734/156371034-13f03f12-1554-4a88-a44b-95ae0ec7e173.png)

스키마를 보면 파티셔닝은 하나로 선언이 되어 이있는 것을 확인 할 수 있다.

![5-2-32](https://user-images.githubusercontent.com/86764734/156371133-92abae1e-6e96-432e-b389-10b884ca4fd1.png)

파티셔닝을 주면 이전에 설명한 것 처럼 저부분을 클릭하면 index가 계속 업데이트 된다. 이 index 안에 들어오지 않는 데이터는 sql로 조회했을 때 조회되지 않는다. 이 점 유의해서 항상 index를 확인해야한다.

![5-2-33](https://user-images.githubusercontent.com/86764734/156371286-0455e09d-2777-4400-b1b8-6b8f9d87e9b6.png)

다시 zeppelin으로 가자

2. 우리가 만든게 단지 danji_user_view_silver! 
glue에 생성된것을 확인 했고, 이제 데이터를 입력을 해야한다.
    
    우리가 생성한 데이블에 insert 할 수 있다.
        
    테이블명은 그대로 카피를 해서 넣는다.
        
    insert overwrite → 이 파티션 기준으로해서  오버라이팅, 만약 데이터가 쌓이다가 에러가 나도 이 파티션 단위로 데이터를 삭제하고 다시 인서트하는 형태. 그래서 오버라이트 옵션을 줘도 테이블 전체가 삭제되는 것이 아니라 파티션 단위로 삭제되고 인서트된다고 이해!
        
    partition : base_date
        
    아래 쿼리는 우리가 위에서 확인한 것!
        
    실행을하면, 파케이로 떨구는 것 보다 7-10배 가까이 hive에서 생성하고 인서트 하는 방식이 속도가 빠르다.

![5-2-34](https://user-images.githubusercontent.com/86764734/156371475-1ebe984e-cfac-4b5b-8fa6-63186a5341e9.png)

3. 다시 그루에 가서 카탈로그 확인해보자
    
    아까 확인했던 danji_user_view_silver에 가서 오른쪽 상단에 파티션보기 클릭

![5-2-35](https://user-images.githubusercontent.com/86764734/156371560-18b62429-774f-43bd-80bf-057f4aba9723.png)

파티션이 생긴 것을 확인 할 수 있다. 

![5-2-36](https://user-images.githubusercontent.com/86764734/156371655-6e45e7ba-7fac-45b2-aab7-dc57fa7dd643.png)

클릭하면 s3로 가서 확인을 한다.

![5-2-37](https://user-images.githubusercontent.com/86764734/156371760-5c66fa0b-1fdd-4e9d-9d5a-326b9ef999f0.png)

4. 이렇게 생성되었을 때 우리가 emr에서 쉽게 조회가능
    
    이 생성된 테이블을 카피한 다음에 sql에서 조회해보자.

![5-2-38](https://user-images.githubusercontent.com/86764734/156371849-364490c8-25b2-4ab1-bb52-98293fb69e67.png)

5. 우리가 실질적으로 알고 싶은것은 단지별로 얼마나 조회했는지!
    
    단지에 대한 마스터가 없다고 가정하에 분석을 하는 것이기 때문에, 37139 단지 id가 제일 조회가 많이 되었다.  
    
    그럼 이 단지는 어느 위치에 있는가? 여러가지 궁금한 것들이 있는데 그 데이터에 대한 작업을 다음에 실습할 것!

![5-2-39](https://user-images.githubusercontent.com/86764734/156371934-5c633e20-b063-433e-95ff-ac9d0a2641ca.png)