## ElasticSerch-logstash 실습 -1

---

실습을 위한 준비는 다 되었다. 우리가 s3에다가 Json형태의 하루치 로그파일을 생성을 했다.

그리고 그것을 logstash를 통해서 띄워놓은 elasticserch에다가 데이터를 보내서 키바나로 분석하는 것 까지 진행.

s3에 있는 것을 우리가 ec2를 하나 띄어서 읽는 것까지 실습해보고 그 데이터를 elasticserch에 보내는 실습 진행할 것.

###  ElasticSerch 확인

---

elasticserch가 빠르게 변화하기 때문에 미리 테스트를 진행해셔서 도메인이 두개

지금 하루치 데이터가 들어가 있어서 클러스터 상태가 노란색 → 클러스터 상태가 좋지 않다는 의미로 노드를 늘려주거나 그런 작업을해야하는 상태이다. 그렇지만 데이터를 보는대 까지는 크게 문제가 없다.

![4-4-1](https://user-images.githubusercontent.com/86764734/160418812-b64eb82f-ecf9-427a-8243-c1890b247c10.png)

1. 초록색 클릭
    
    여기서 중요하게 볼 것이 오른쪽에 오픈서치대시보드 url인데 이게 키바나이다. 
    클릭하면 키바나로 로그인하게 되있다.
    
    도메인 엔드포인트는 우리가 이제 프로그램에서 데이터를 넣기 위한 엔트포인트이다.

    ![4-4-2](https://user-images.githubusercontent.com/86764734/160418857-c2d8a85c-ed69-4afc-a487-1a087c133ae0.png)

2. 키바나 로그인 해보자
    
    앞에서 설정해주었던 아이디와 패스워드 기억해두자!

    ![4-4-3](https://user-images.githubusercontent.com/86764734/160418891-a403b3d6-270b-4359-a64c-fa84a284f505.png)

    Explore on my own 클릭

    ![4-4-4](https://user-images.githubusercontent.com/86764734/160418928-e80f5ca1-8a1f-4a98-b996-8050d6423851.png)

3. 키바나 접속
    
    지금은 여기서 주어지는 샘플데이터가 있어서 그것을 클릭하면 기존에 샘플 테이터로 대시보드를 볼 수 있고,
    
    우리가 이제 데이터를 넣을 것이다.

    ![4-4-5](https://user-images.githubusercontent.com/86764734/160418976-7f74ad8f-082f-4434-bf6c-b3b382c4b3c2.png)

    들어온 데이터가 기본적으로 어떤 형태인지 확인하기 위해서 인덱스를 구성하고 그 다음에 디스커버리라는 임시적으로 데이터를 볼 수 있는 것으로 확인하고 대시보드를 구성하면 된다.

    ![4-4-6](https://user-images.githubusercontent.com/86764734/160419024-2456691a-06a0-4ddf-8416-8753a72a42ec.png)

    그러면 이제 키바나는 준비가 됐다!

### cli-logstash

---

logstash를 통해 데이터를 elasticserch에다가 넣어보자

1. ec2 접속
    
    logstash 인스턴스 체크하고 연결

    ![4-4-7](https://user-images.githubusercontent.com/86764734/160419073-f3de862a-c0a5-4134-b1f9-11f444ed3f4e.png)

    연결 정보 카피 

    ![4-4-8](https://user-images.githubusercontent.com/86764734/160419115-884e4d4b-8d45-40c4-80df-895bd7eed2a8.png)

2. cli에다가 붙여넣는다
    
    실행위치는 keypair

    ![4-4-9](https://user-images.githubusercontent.com/86764734/160419153-ac9e2f53-280e-4564-9a22-bfb5132e93e3.png)

3. 리스트 확인
    
    지난 시간에 logstas까지 설치했다.

    ![4-4-10](https://user-images.githubusercontent.com/86764734/160419180-95e099bf-9e66-4cd2-9ef6-7b4b6e445bce.png)

4. 먼저 producer_test.conf 

    ![4-4-11](https://user-images.githubusercontent.com/86764734/160419255-bdcde247-1385-4827-ae23-57df0c5a0c79.png)

    인풋값으로 s3에 있는 파일을 받겠다.

    서비스별로 권한을 부여를 하면 s3에 있는 데이터를 읽을 수 있다.
    그것을 이제 cli 콘솔에서 할 수 있는다. 이것은 약간 어플리케이션 레벨이기 때문에 엑세스키와 시큐리티 키를 넣지 않으면 실행이 되지 않는다. 그래서 키 정보를 넣어주면 된다.

    프리픽스는 s3에 버켓위치를 말하는 것. 서버(서브?)폴더라고 알고 있으면 된다.

    temp/json 이라고 해두면 Json 폴더 밑에 있는 모든 파일이 대상이 된다. 

    그리고 필터로 Json형태에 소스를 받아서 그대로 쏘는 형태가 될 것. 
    여기에서 필요없는 컬럼을 버리거나 여러가지 작업들을 실행할 수 있다.  
    그래서 전에도 실습에 있어서 강조하는 것은 스텝별로 한번에 진행해도 좋은데 그것 보다 테스트를 진행하고 그것이 성공하면 producer.conf로 구성을 해서 elasticserch로 보내는 작업을 하는게 더 확실하다.

    ![4-4-12](https://user-images.githubusercontent.com/86764734/160419284-5959f91c-4cfd-4020-aafc-3fc3388059e7.png)

5. s3에 있는 데이터를 읽어오는 작업을 진행
    
    정상적으로 읽히는지 확인하는게 중요하다. 
    
    정확하게 데이터가 찍히면 그때 데이터를 elasticserch에 쌓아야한다.

    ![4-4-13](https://user-images.githubusercontent.com/86764734/160419336-618d9258-2916-4b53-9e3e-395544e81448.png)

    확인해보니 에러가 발생..

    그 이유는 엑세스키랑 시큐리티를 좀 바꿔놨다. 해킹의 원인이 될 수 있어서.

    깃으로 공유를 해야하기 때문에 변경을 해서 그런것 같다.

    ![4-4-14](https://user-images.githubusercontent.com/86764734/160419357-a79a1a0c-48c2-4ed8-ad0a-2ecc487791d2.png)

6. 엑세스키와 시큐리티키 수정
    
    test가 아닌 producer.conf로 가서 엑세스키와 시큐리티 키를 복사해오자~
    ![4-4-15](https://user-images.githubusercontent.com/86764734/160419416-29920b7e-11ad-4eb5-b249-fcc590227a01.png)

    !vi 입력해서 들어가자 기존의 내용을 수정!

    ![4-4-16](https://user-images.githubusercontent.com/86764734/160419458-37449d30-d61c-45c6-a5e3-8e8f3c71ec2c.png)

7. 그리고 다시한번 실행해보자.
    
    그래서 항상 테스트를 통해서 확인을 하고 스텝별로 해야한다.
    
    실전에는 더 많은 문제가 발생을 한다. 그렇기 때문에 항상 단계별로 체크를 해야한다.

    ![4-4-18](https://user-images.githubusercontent.com/86764734/160419502-64fc928f-5405-449d-89c6-7e068f08acac.png)

    elasticserch에 메세지가 떨궈지는 것을 좀 보면 중간에 인덱스가 띄어지는 부분이 있다. 
    인덱스라는게 뭐냐면 로그가 이제 만개가 있다 그러면 어디까지 elasticserch에다가 발생을 했는지 대해서 신스디비에다가 저장을 한다. 

    그래서 보면 지금 우리가 sql을 통해서 spark를 통해서 데이터를 생성을 했는데 거기에 없는 버전이라는 것이 들어왔고, timestamp라는 것이 있는데 이것은 현재시간을 찍어주는 것 utf타임

    base_dt는 우리가 변환을 해서 넣줬다. 그래서 2021 12 26일 인것을 확인 할 수 있다. 

    그 다음에 스파크에 있는 컬럼의 헤더에 해당되는 부분이 있는데 그런 부분이 다 있고 메세지 에는 전체부분을 넣어주는 것 이런 부분은 뺴줄수도 있다. 
    이 데이터가 정확하게 찍히는지 확인한 후, 그런 다음에 우리가 띄어놓은 elasticserch 엔드포인트를 넣어주고 이 데이터를 보내보도록 할 것.

    ![4-4-19](https://user-images.githubusercontent.com/86764734/160419569-eab0754a-57b9-4da3-b80d-4689742fabe2.png)


8. vi producer.conf

    vi producer.conf로 카피를 한 다음에 얘를 넣어주는데 

    ![4-4-20](https://user-images.githubusercontent.com/86764734/160419641-f1f7bc4a-1516-420d-ac56-554b86cf3675.png)

    지금 여기서는 뮤터블이라는 테그를 더 넣어주고, logstash에도 여러가지 기능이 있어서 우리가 좀 더 가공을 통해서 데이터를 elasticserch로 보낼 수 있다. 

    아까 불필요하다 싶던 내용들을 빼고 싶으면 romove로 뺴주면 된다. 왜냐면 메세지에 데이터가 너무 많기 때문에 많은 데이터를 가져가기 부담스럽다 싶으면 뺴주면 된다.

    ![4-4-21](https://user-images.githubusercontent.com/86764734/160419668-5cff4354-5495-4df9-8f14-be79ecf31cd9.png)





